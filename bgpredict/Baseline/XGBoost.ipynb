{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "852e17eb-904c-4f04-b450-83a31ff0459f",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a6ebc25-76ee-4c46-9715-a2293ef521cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from xgboost import plot_importance\n",
    "from bgpredict.helpers import S3Connection\n",
    "from dotenv import load_dotenv\n",
    "import multiprocessing\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.plots import plot_objective, plot_histogram\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb78ee1c-bd05-4d5f-a8ca-589462e90664",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')\n",
    "load_dotenv()\n",
    "os.chdir('./Baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36fe3377-2785-46b3-80ba-2b392418fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # Drop rows with no Y value\n",
    "    df = df.dropna(subset='bg')\n",
    "\n",
    "    # Fill nulls (lag BG values) with 0 to indicate data is unavailable\n",
    "    print(f\"Null values to be filled by column:\")\n",
    "    nulls = df.isna().sum()\n",
    "    null_idx = list(nulls.index)\n",
    "    vals = list(nulls)\n",
    "    for col, val in list(zip(null_idx, vals)):\n",
    "        if val > 0:\n",
    "            print(col, val)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # Sort by timestamp\n",
    "    current_len = len(df)\n",
    "    df = df.loc[~(df['timestamp_clean'].apply(type) == int), :]\n",
    "    print(f\"Dropping {current_len - len(df)} invalid timestamps\")\n",
    "    df = df.sort_values(by=\"timestamp_clean\")\n",
    "\n",
    "    # Set index to time_stamp_clean\n",
    "    df.index = df['timestamp_clean']\n",
    "    df = df.drop(labels=['timestamp_clean'], axis=1)\n",
    "\n",
    "    # Drop first row by subject which has data quality issues\n",
    "    df = df[df.groupby('subjectid').cumcount() > 0]\n",
    "\n",
    "    # Drop columns that are indices, irrelevant, or capture in OHE variables\n",
    "    drop_cols = ['timestamp', 'date', 'time']\n",
    "    df = df.drop(labels=drop_cols, axis=1)\n",
    "\n",
    "    # One hot Encode Weekdays\n",
    "    weekdays = np.unique(df['weekday'])\n",
    "    ohe_weekdays = [f\"ohe_{day}\" for day in weekdays]\n",
    "    df[ohe_weekdays] = pd.get_dummies(df.weekday)\n",
    "    df = df.drop(labels=\"weekday\", axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f04129-4f58-4c02-bfb5-06f5c731ed6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values to be filled by column:\n",
      "timestamp 2\n",
      "timestamp_clean 2\n",
      "date 2\n",
      "time 2\n",
      "weekday 2\n",
      "hour 2\n",
      "minute 2\n",
      "datediff_currentbg_lastbg_inseconds 3\n",
      "bg_lag_1 88\n",
      "bg_lag_2 86\n",
      "bg_lag_3 77\n",
      "bg_lag_4 85\n",
      "bg_lag_5 87\n",
      "bg_lag_6 75\n",
      "bg_lag_7 77\n",
      "bg_lag_8 85\n",
      "bg_lag_9 94\n",
      "bg_lag_10 83\n",
      "bg_lag_11 93\n",
      "bg_lag_12 96\n",
      "Dropping 2 invalid timestamps\n"
     ]
    }
   ],
   "source": [
    "location = f\"postgresql://postgres:{os.environ.get('db_password')}@{os.environ.get('db_location')}\"\n",
    "engine = create_engine(location)\n",
    "conn = engine.connect()\n",
    "raw_df = pd.read_sql(\"select * from public.vw_final_dataset limit 10000\", conn)\n",
    "clean_df = clean_data(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3883133e-c95c-4a05-9cff-f59a25e8190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cv_splits(df):\n",
    "    df = df.reset_index()\n",
    "    subjects = np.unique(df['subjectid'])\n",
    "    cv_splits = []\n",
    "    for subject in subjects:\n",
    "        train_idx = np.array(df.loc[(df['subjectid']==subject) & (df['train_set']==1)].index)\n",
    "        val_idx = np.array(df.loc[(df['subjectid']==subject) & (df['validation_set']==1)].index)\n",
    "        if len(train_idx) < 5 or len(val_idx) < 5:\n",
    "            continue \n",
    "        split = (train_idx, val_idx)\n",
    "        cv_splits.append(split)\n",
    "    return cv_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5619726-301a-4d08-85c6-98e6b2745530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_cv_split(df):\n",
    "    clean_df_reset = df.reset_index()\n",
    "    \n",
    "    train_X = clean_df_reset.loc[(clean_df_reset['train_set'] == 1) | (clean_df_reset['validation_set'] == 1) , clean_df_reset.columns != 'bg']\n",
    "    train_y = clean_df_reset.loc[(clean_df_reset['train_set'] == 1) | (clean_df_reset['validation_set'] == 1), clean_df_reset.columns == 'bg']\n",
    "    \n",
    "    test_X = clean_df_reset.loc[(clean_df_reset['test_set'] == 1), clean_df_reset.columns != 'bg']\n",
    "    test_y = clean_df_reset.loc[(clean_df_reset['test_set'] == 1), clean_df_reset.columns == 'bg']\n",
    "    cv_splits = create_cv_splits(train_X)\n",
    "\n",
    "    drop_cols = ['train_set', 'validation_set', 'test_set', 'subjectid', 'entryid', 'timestamp_clean']\n",
    "    train_X = train_X.drop(labels=drop_cols, axis=1)\n",
    "    test_X = test_X.drop(labels=drop_cols, axis=1)\n",
    "    \n",
    "    return train_X, train_y,test_X, test_y, cv_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c304673-2e20-4ba7-be78-1c00f35a982d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, 0.6, 0.7, 0.8, 0.9, 1.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.5 + i/10 for i in range(0, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1738699-2207-4e18-aa90-98751c0fb981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Finished in: 0:00:36.473867\n",
      "val. score: 0.7407450921021447\n",
      "test score: 0.8768891664673926\n",
      "best params: OrderedDict([('colsample_bylevel', 0.7), ('colsample_bynode', 0.7), ('colsample_bytree', 0.7), ('gamma', 1.0), ('learning_rate', 0.75), ('max_delta_step', 2), ('max_depth', 4), ('min_child_weight', 1.0), ('n_estimators', 250), ('reg_alpha', 0.1), ('reg_lambda', 0.2), ('subsample', 0.8)])\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, test_X, test_y, cv_splits = train_test_cv_split(clean_df[1500:2500])\n",
    "\n",
    "n_cpu = os.cpu_count()\n",
    "\n",
    "param = {'max_depth': 2,\n",
    "      \"learning_rate\": [0.1,0.2,0.3], #\n",
    "      \"gamma\" : 1, # 0 to inf; minimum loss reduction required to make a partition. Larger is more conservative\n",
    "      \"min_child_weight\": 3, # minimum hessian weight of leaf nodes, o to inf\n",
    "      \"n_estimators\": 2, #equivalent to num rounds\n",
    "      \"reg_lambda\": 5, # l2, 0 to inf\n",
    "      \"reg_alpha\": 0.1, # l1, 0 to inf\n",
    "      \"n_estimators\":range(50,400,40),\n",
    "     }\n",
    "\n",
    "start = datetime.now()\n",
    "# params_test = {\"n_estimators\":range(50,400,40)}\n",
    "params_test = {\"max_depth\": [i for i in range(2, 20, 2)],\n",
    "               \"learning_rate\": [0.01, 0.1,0.2,0.3, 0.5, 0.75, 1],\n",
    "               \"n_estimators\":[x for x in range(50,400,50)],\n",
    "               \"reg_alpha\": [0,0.0001,0.001,0.01,0.1],\n",
    "               \"reg_lambda\":[0,0.0001,0.001,0.01,0.1, 0.2, 0.3, 0.5, 0.75, 1, 2, 3],\n",
    "               \"gamma\": [0, 0.25, 0.5, 1.0, 2, 4, 8, 16],\n",
    "               \"min_child_weight\": [0.25, 0.5, 1, 3, 5, 7],\n",
    "               \"colsample_bytree\": [0.5 + i/10 for i in range(0, 6)],\n",
    "               \"colsample_bylevel\": [0.5 + i/10 for i in range(0, 6)],\n",
    "               \"colsample_bynode\": [0.5 + i/10 for i in range(0, 6)],\n",
    "               \"subsample\": [0.5 + i/10 for i in range(0, 6)],\n",
    "               \"max_delta_step\": [i for i in range(0,11)]\n",
    "              }\n",
    "\n",
    "regressor = xgboost.XGBRegressor(eval_metric=\"rmse\", verbosity=0)\n",
    "\n",
    "cv_search = BayesSearchCV(estimator=regressor,\n",
    "                              search_spaces = params_test,\n",
    "                              n_iter=24,\n",
    "                              n_jobs=n_cpu, \n",
    "                              cv=cv_splits,\n",
    "                              verbose=1)\n",
    "\n",
    "cv_search.fit(train_X, train_y)\n",
    "print(f'Finished in: {datetime.now()-start}')\n",
    "\n",
    "print(\"val. score: %s\" % cv_search.best_score_)\n",
    "print(\"test score: %s\" % cv_search.score(test_X, test_y))\n",
    "print(\"best params: %s\" % str(cv_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f398cf26-d2fe-40cc-bed3-09b64ed28f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_objective(cv_search.optimizer_results_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c959262-73d2-4481-be6b-bab9d0c1dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = cv_search.best_estimator_\n",
    "best_estimator.save_model('./model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d12d5-e6e5-4c4b-897a-17f6cb3edd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"{datetime.now().time()}_L{len(train_X)}\".replace(\":\", \".\")\n",
    "location = f\"models/xgboost/{model_name}\"\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d91ae60-bfdd-48c4-a081-885057273c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_conn = S3Connection()\n",
    "bucket = s3_conn.bucket_name\n",
    "with open('./model.json') as f:\n",
    "    model = json.load(f)\n",
    "    s3_conn.s3_client.put_object(Bucket=bucket, Key=location, Body=json.dumps(model))\n",
    "os.remove(\"./model.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry_kernel",
   "language": "python",
   "name": "poetry_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
