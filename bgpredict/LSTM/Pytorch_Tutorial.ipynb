{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0568830-1970-4310-9b21-22364064b944",
   "metadata": {},
   "source": [
    "# LSTM Exploration\n",
    "\n",
    "Notebook for figuring out how to run an LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd54f6a-77cb-49ff-898d-0c1649fbacbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import pandas as pd\n",
    "# from dotenv import load_dotenv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ec0ecb-51d4-4e04-af5e-1f2421b4364d",
   "metadata": {},
   "source": [
    "## Sentdex PyTorch Tutorial\n",
    "[Sentdex's Pytorch series](https://www.youtube.com/watch?v=BzcBsTou0C0). This is more of an intro to Pytorch than LSTM, but that's fine for now.\n",
    "\n",
    "### Video 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42d57c91-39cf-4150-9c26-1aac02fea425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  3.])\n"
     ]
    }
   ],
   "source": [
    "# Tensors are just numpy arrays\n",
    "x = torch.Tensor([5,3])\n",
    "y = torch.Tensor([2,1])\n",
    "print(x*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487e77c-54e8-4f97-bc20-0dd72c24bf28",
   "metadata": {},
   "source": [
    "Torch is similar to numpy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58c51598-3eef-4c40-a524-6472789968f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros([2,5])\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae83f984-05a1-4356-808a-73b2f8f3ffb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2675, 0.2013, 0.9053, 0.1067, 0.1656],\n",
       "        [0.1622, 0.8483, 0.8833, 0.4758, 0.7938]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand([2,5]) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6306b1b9-21cf-4e68-8255-a5954e847194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2675, 0.2013, 0.9053, 0.1067, 0.1656, 0.1622, 0.8483, 0.8833, 0.4758,\n",
       "         0.7938]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape equivalent\n",
    "y.view([1,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef5c070-7eb0-459e-b43d-5d3a4ac0c91d",
   "metadata": {},
   "source": [
    "### Video 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7df3e67-b8ff-4792-82b9-e86222e23bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdbc68c-dba7-44b4-a7f0-3036be091984",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train=True, download=True, \n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"\", train=False, download=True, \n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5267057c-9c66-4517-abb1-d89423f369a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best batch size is typically between 8 and 64 (bigger batch sizes train faster)\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f923832-566d-4fb2-b7ae-e89af3c4e79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([4, 8, 5, 8, 7, 9, 6, 1, 6, 8])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "606e2969-6cf8-44d5-bc1a-964d00332f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = data[0][0], data[1][0] #[1][0] is the y value \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d845ad7-871e-421e-8818-99e625c128ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a41fefde-5a0e-4e4f-a6c1-05d7b10bc017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  # pip install matplotlib\n",
    "\n",
    "# Reshape into 28 * 28 image \n",
    "plt.imshow(data[0][0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55930883-bc77-4786-8313-6f0adc6350ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba02f32d-517f-4230-9f12-8530d137fc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0][0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2800df2-8912-4e46-be8b-e558f2db5ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n",
      "0: 9.871666666666666%\n",
      "1: 11.236666666666666%\n",
      "2: 9.93%\n",
      "3: 10.218333333333334%\n",
      "4: 9.736666666666666%\n",
      "5: 9.035%\n",
      "6: 9.863333333333333%\n",
      "7: 10.441666666666666%\n",
      "8: 9.751666666666667%\n",
      "9: 9.915000000000001%\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "\n",
    "for data in trainset:\n",
    "    Xs, ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total += 1\n",
    "\n",
    "print(counter_dict)\n",
    "\n",
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i]/total*100.0}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa58af4a-11b7-4c2d-a962-2e0075d62631",
   "metadata": {},
   "source": [
    "### Video 3 - Building the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fdcfe17-3198-4c9f-9f4d-438c6e780b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relatively interchangeable. NN is object oriented. Functional is, well, functional\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccb6b3f8-1b13-4bb0-b0f8-b95d908f090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Fully connected layer 1, 784 = 28*28\n",
    "        # 64 output dimension\n",
    "        self.fc1 = nn.Linear(784, 64) \n",
    "        self.fc2 = nn.Linear(64, 64) \n",
    "        self.fc3 = nn.Linear(64, 64) \n",
    "        self.fc4 = nn.Linear(64, 10) # ten classes \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # defines how the data flows through our network\n",
    "        \n",
    "        # Run RELU activation function across the entire layer (keeps outputs from exploding)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        # Do not fun RELU on the output layer \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1) # softmax is for multiclass, dim=1 specifies that you apply softmax across the output tensor \n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b27cdb8b-471f-4055-ba16-9e0a771b2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(-1, 28*28) #-1 specifies that the data will be an unknown shape. (1,28*28) would work too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4036eb04-3b41-4762-b7f8-15a9c79e144a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1887, -2.3954, -2.3262, -2.2918, -2.1752, -2.3876, -2.4088, -2.3453,\n",
       "         -2.2136, -2.3271]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output= net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5147ea6f-ae86-4aa1-b2f8-dc32622ed9f0",
   "metadata": {},
   "source": [
    "### Video 4 - Training the model\n",
    "...continued from prior video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44dd6642-53e2-4a4d-8662-ab789cd769af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1069, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# net.parameters() means that we optimize every parameter \n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS=3 # number of passes through our dataset\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        # data is a batch of featuresets and labels\n",
    "        X,y = data\n",
    "        # zero grad helps deal with accumulating gradients. More info here: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "        net.zero_grad() \n",
    "        output = net(X.view(-1, 28*28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward() # back propogation! \n",
    "        optimizer.step() # performs back propr\n",
    "        \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eabb0924-c2aa-4647-b3cc-a93471f1a188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.977\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Torch no grad means we don't calculate anything here. \n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X,y = data\n",
    "        output = net(X.view(-1, 784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1 \n",
    "            total +=1 \n",
    "print(f\"Accuracy: {round(correct/total, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f6e4462-dfaa-4ed5-ab67-05c59ec778c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.imshow(X[4].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1a4151d-8db4-4c09-97a4-0e9eb0e6abe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[4].view(-1, 784))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7fdf42-80bb-4056-99df-8a818cc160cd",
   "metadata": {},
   "source": [
    "### Video 5 -- Covnet Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d8ede2-d3d6-4293-850c-a943f9f9c883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48ac48f8-b42e-444d-a7ac-727e8da43904",
   "metadata": {},
   "source": [
    "### Video 6 -- Training Convnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ccc570-7c99-4e1e-a4b1-fee8e4c8e148",
   "metadata": {},
   "source": [
    "### Video 7 -- Using the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642e936e-b34b-4ec1-95ba-d5a68f15ec4d",
   "metadata": {},
   "source": [
    "Make sure you install openssl and/or ssh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33119097-3290-4aa7-9192-f91c1c0a4ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on CPU\n"
     ]
    }
   ],
   "source": [
    "# check for cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print('on GPU')\n",
    "else: \n",
    "    device = torch.device('cpu')\n",
    "    print('on CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db1d0794-3677-4925-80bf-24a419f176d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8581e6-0b6b-48db-80cd-0e7e344e1ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95f0d77d-b8ee-47f2-b427-48da167bc002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the net onto the GPU\n",
    "net.to(device) # OR\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54547d7f-8e56-44da-8860-44bf12cb605e",
   "metadata": {},
   "source": [
    "Additional instructions for getting the data for this example can be found [here](https://pythonprogramming.net/gpu-deep-learning-neural-network-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09649685-4979-4b43-a8ba-6fad9c508b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from convnet videos that I've not read fully \n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "REBUILD_DATA = False # set to true to one once, then back to false unless you want to change something in your training data.\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50\n",
    "    CATS = \"PetImages/Cat\"\n",
    "    DOGS = \"PetImages/Dog\"\n",
    "    TESTING = \"PetImages/Testing\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"jpg\" in f:\n",
    "                    try:\n",
    "                        path = os.path.join(label, f)\n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])  # do something like print(np.eye(2)[1]), just makes one_hot \n",
    "                        #print(np.eye(2)[self.LABELS[label]])\n",
    "\n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "                        elif label == self.DOGS:\n",
    "                            self.dogcount += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                        #print(label, f, str(e))\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print('Cats:',dogsvcats.catcount)\n",
    "        print('Dogs:',dogsvcats.dogcount)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) # input is 1 image, 32 output channels, 5x5 kernel / window\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "\n",
    "        x = torch.randn(50,50).view(-1,1,50,50)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
    "        self.fc2 = nn.Linear(512, 2) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n",
    "\n",
    "    def convs(self, x):\n",
    "        # max pooling over 2x2\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) # bc this is our output layer. No activation here.\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()\n",
    "\n",
    "training_data = np.load(\"training_data.npy\", allow_pickle=True)\n",
    "print(len(training_data))\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n",
    "X = X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "VAL_PCT = 0.1  # lets reserve 10% of our data for validation\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "\n",
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbd08430-65e1-4809-be99-15965a03d6be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22856/1895128165.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch: {epoch}, loss: {loss}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22856/1895128165.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mloss_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0mbatch_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "def train(net):\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 3\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in range(0, len(train_X), BATCH_SIZE): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n",
    "            #print(f\"{i}:{i+BATCH_SIZE}\")\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            net.zero_grad()\n",
    "\n",
    "            optimizer.zero_grad()   # zero the gradient buffers\n",
    "            outputs = net(batch_X)\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()    # Does the update\n",
    "\n",
    "        print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
    "\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdff4e4d-b61b-4cf2-bd7e-3969ea5a2532",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22856/2017401400.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_X' is not defined"
     ]
    }
   ],
   "source": [
    "test_X.to(device)\n",
    "test_y.to(device)\n",
    "\n",
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(test_X))):\n",
    "            real_class = torch.argmax(test_y[i]).to(device)\n",
    "            net_out = net(test_X[i].view(-1, 1, 50, 50).to(device))[0]  # returns a list, \n",
    "            predicted_class = torch.argmax(net_out)\n",
    "\n",
    "            if predicted_class == real_class:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "    print(\"Accuracy: \", round(correct/total, 3))\n",
    "\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257b56eb-5a04-477d-83eb-7db6c2f21b4c",
   "metadata": {},
   "source": [
    "### Video 8 - Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02b59462-2043-4a51-bd93-7d5cf9be7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_pass(X, y, train=False):\n",
    "    if train:\n",
    "        net.zero_grad()\n",
    "    outputs = net(X)\n",
    "    matches = [torch.argmax(i) == torch.argmax(j) for i, j in zip(outputs, y)]\n",
    "    acc = matches.count(True)/len(matches)\n",
    "    loss = loss_function(outputs, y)\n",
    "    \n",
    "    if train:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d30a365-2381-4841-b3c4-2dfde96e4067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(size=32):\n",
    "    random_start = np.random.randint(len(test_X)-size)\n",
    "    X, y = text_X[random_start:random_start+size], test_y[random_start:random_start+size]\n",
    "    with torch.no_grad():\n",
    "        val_acc, val_loss = fwd_pass(X.view(-1, 1, 50, 50).to(device), y.to(device))\n",
    "    return val_acc, val_loss\n",
    "\n",
    "val_acc, val_loss = test(size-32)\n",
    "print(val_acc, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee0f135-79ee-47d1-b4b0-bd57ad1e43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "MODEL_NAME = f\"model-{int(time.time())}\"\n",
    "\n",
    "net = Net().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSEloss()\n",
    "\n",
    "def train():\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 1\n",
    "    with open('model.log', \"a\") as f:\n",
    "        for epoch in range(EPOCHS):\n",
    "            for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "                batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50).to(device)\n",
    "                batch_y = train_y[i:i+BATCH_SIZE].to(device)\n",
    "                \n",
    "                acc. loss = fwd_pass(batch_X, batch_y, train=True)\n",
    "                for i % 50 == 0:\n",
    "                    val_acc, val_loss = test(size=100)\n",
    "                    f.write(f\"{MODEL_NAME}, {round(time.time(), 3)}, {round(float(acc),2)}, {round(float(loss, 4))}, {round(float(val_acc),2)}, {round(float(val_loss, 4))} \\n\")\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8a4b29-8fd2-4d07-a01e-dcea547c431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be done just fine with pandas \n",
    "def create_acc_loss_graph(model_name):\n",
    "    contents = open('model.log', 'r').read().split('\\n')\n",
    "    times = []\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    val_accs = []\n",
    "    val_losses = []\n",
    "    for c in contents:\n",
    "        if model_name in c:\n",
    "            name, timestamp, acc, loss, val_acc, val_loss = c.split(',')\n",
    "            \n",
    "            time.append(float(timestamp))\n",
    "            accuracies.append(float(acc))\n",
    "            losses.append(float(loss))\n",
    "            val_accs.append(float(val_acc))\n",
    "            val_losses.append(float(val_loss))\n",
    "    fig = plt.figure()\n",
    "    ax1 = plt.subplot2grid((2,1), (0,0))\n",
    "    ax1 = plt.subplot2grid((2,1), (1,0), sharex=ax1)\n",
    "    \n",
    "    ax1.plot(times, accuracies, label='acc')\n",
    "    ax1.plot(times, val_accs, label='val_acc')\n",
    "    ax1.legend(loc=2)\n",
    "    \n",
    "    # just repeat above for losses \n",
    "    ax2.plot()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
