{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8cc12c-cc60-4437-b701-b084a2927efc",
   "metadata": {},
   "source": [
    "# LSTM Production\n",
    "Notebook to put LSTM into production on AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c451c-7a81-4ec8-9ad2-13782a1acab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from functools import partial\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b83bf59-f87e-4919-b48f-162e7964ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2900b8b7-1b4b-471b-aeca-7d0d4b7dbeae",
   "metadata": {},
   "source": [
    "## Data Loading, Cleaning, and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de5d23d-48a6-4c74-8a14-707bfd49dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing Functions\n",
    "def load_data_to_df():\n",
    "    location = f\"postgresql://postgres:{os.environ.get('db_password')}@{os.environ.get('db_location')}\"\n",
    "    engine = create_engine(location)\n",
    "    conn = engine.connect()\n",
    "    raw_df = pd.read_sql(\"select * from public.vw_final_dataset where subjectid = 2033176\", conn)\n",
    "    return raw_df\n",
    "\n",
    "def clean_data(df):\n",
    "    # Drop rows with no Y value\n",
    "    df = df.dropna(subset='bg')\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values(by=\"timestamp_clean\")\n",
    "    \n",
    "    # Set index to time_stamp_clean\n",
    "    df.index = df['timestamp_clean']\n",
    "    df = df.drop(labels=['timestamp_clean'], axis=1)\n",
    "    \n",
    "    # Drop first row by subject which has data quality issues\n",
    "    df = df[df.groupby('subjectid').cumcount() > 0] \n",
    "    \n",
    "    # Drop columns that are indices, irrelevant, or capture in OHE variables\n",
    "    drop_cols = ['subjectid', 'entryid', 'timestamp', 'date', 'time']\n",
    "    df = df.drop(labels=drop_cols, axis=1)\n",
    "    \n",
    "    # Fill nulls (lag BG values) with 0 to indicate data is unavailable\n",
    "    print(f\"Null values to be filled by column:\")\n",
    "    nulls = df.isna().sum()\n",
    "    null_idx = list(nulls.index)\n",
    "    vals = list(nulls)\n",
    "    for col, val in list(zip(null_idx, vals)):\n",
    "        if val > 0:\n",
    "            print(col,val)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    # One hot Encode Weekdays\n",
    "    weekdays = np.unique(df['weekday'])\n",
    "    ohe_weekdays = [f\"ohe_{day}\" for day in weekdays]\n",
    "    df[ohe_weekdays] = pd.get_dummies(df.weekday)\n",
    "    df = df.drop(labels=\"weekday\", axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def split_and_scale(df):\n",
    "    # train/val/test split\n",
    "    train_df = df.loc[df['train_set'] ==1, :]\n",
    "    val_df = df.loc[df['validation_set'] ==1, :]\n",
    "    test_df = df.loc[df['test_set'] == 1, :] \n",
    "    \n",
    "    # Extract y vars\n",
    "    train_y = train_df['bg']\n",
    "    val_y = val_df['bg']\n",
    "    test_y = test_df['bg']\n",
    "    \n",
    "    # Drop non-X columns\n",
    "    drop_cols = ['train_set', 'validation_set', 'test_set', 'bg']\n",
    "    train_df = train_df.drop(labels=drop_cols, axis=1)\n",
    "    val_df = val_df.drop(labels=drop_cols, axis=1)\n",
    "    test_df = test_df.drop(labels=drop_cols, axis=1)\n",
    "    \n",
    "    # Select Scaling columns (i.e. don't scale one hot encoded variables)\n",
    "    ohe_cols = train_df.columns[train_df.columns.str.contains('ohe')]\n",
    "    scaling_cols = train_df.columns.difference(ohe_cols)\n",
    "    print(f\"{len(ohe_cols)} one hot encoded columns \")\n",
    "    print(f\"{len(scaling_cols)} scaled columns\")\n",
    "    \n",
    "    # Fit Scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_df[scaling_cols])\n",
    "    \n",
    "    # Perform Scaling \n",
    "    train_array = scaler.transform(train_df[scaling_cols])\n",
    "    val_array = scaler.transform(val_df[scaling_cols])\n",
    "    test_array = scaler.transform(test_df[scaling_cols])\n",
    "    \n",
    "    # Recombine Scaled Data into DataFrame Format \n",
    "    train_scaled_df = pd.DataFrame(train_array, columns=scaling_cols, index=train_df.index)\n",
    "    val_scaled_df = pd.DataFrame(val_array, columns=scaling_cols, index=val_df.index)\n",
    "    test_scaled_df = pd.DataFrame(test_array, columns=scaling_cols, index=test_df.index)\n",
    "    \n",
    "    train_df = pd.concat([train_scaled_df, train_df.loc[:,ohe_cols], train_y], axis=1)\n",
    "    val_df = pd.concat([val_scaled_df, val_df.loc[:,ohe_cols], val_y], axis=1)\n",
    "    test_df = pd.concat([test_scaled_df, test_df.loc[:,ohe_cols], test_y], axis=1)\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def df_to_Xy_tensors(df, window_size=12):\n",
    "    X = []\n",
    "    y = []\n",
    "    num_features = len(df.columns) - 1\n",
    "    for idx in range(window_size, len(df)-window_size):\n",
    "        window_df = df.iloc[idx-window_size:idx]\n",
    "        X.append(window_df.loc[:, df.columns != 'bg'].values)\n",
    "        # The first element is the y value associated with the sequence of X values \n",
    "        y.append(window_df['bg'].iloc[0])\n",
    "        \n",
    "    X_tensor = torch.cat([torch.tensor(i).float() for i in X]).view(len(X), window_size, num_features)\n",
    "    y_tensor = torch.tensor(y).float()\n",
    "    return X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264cd4d-d0d0-49d0-9bc2-77953a8a2e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=8, num_lstm_layers=1, dropout=0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_lstm_layers,\n",
    "                            dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = self.fc1(lstm_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379cbee4-b334-4166-957b-94a0f545a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(config, checkpoint_dir=None):\n",
    "\n",
    "    # Load and clean data\n",
    "    print('Loading Data')\n",
    "    raw_df = load_data_to_df()\n",
    "\n",
    "    print('Processing Data')\n",
    "    clean_df = clean_data(raw_df)\n",
    "    train_df, val_df, test_df = split_and_scale(clean_df)\n",
    "    \n",
    "    # Process data for a given window_size\n",
    "    window_size = config['window_size']\n",
    "    train_X, train_y = df_to_Xy_tensors(train_df, window_size=window_size)\n",
    "    val_X, val_y = df_to_Xy_tensors(val_df, window_size=window_size)\n",
    "    test_X, test_y = df_to_Xy_tensors(test_df, window_size=window_size)\n",
    "    print(\"Data processing finished\")\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "    \n",
    "    print(f\"DEVICE: {device}\")\n",
    "    train_X.to(device)\n",
    "    train_y.to(device)\n",
    "    val_X.to(device)\n",
    "    val_y.to(device)\n",
    "    \n",
    "    # Configure the network and send it to the device\n",
    "    # Width of the dataframe - 1 (y variable) is feature set size \n",
    "    input_size = train_df.shape[1]-1\n",
    "    net = Net(input_size=input_size,\n",
    "              hidden_size=config['hidden_size'],\n",
    "              num_lstm_layers=config['num_lstm_layers'],\n",
    "              dropout=config['dropout'])\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        net = nn.DataParallel(net)     \n",
    "    net.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    # Checkpoint Dir Stuff -- handled by Tune \n",
    "    if checkpoint_dir:\n",
    "        checkpoint = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        model_state, optimizer_state = torch.load(checkpoint)\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)    \n",
    "    \n",
    "    # train\n",
    "    BATCH_SIZE = config['batch_size']\n",
    "    for epoch in range(config['epoch']):\n",
    "        epoch_start = datetime.now()\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        running_loss = 0\n",
    "        epoch_steps = 0\n",
    "        for i in range(0, len(train_X)-BATCH_SIZE, BATCH_SIZE):\n",
    "            X = train_X[i:i+BATCH_SIZE]\n",
    "            y = train_y[i:i+BATCH_SIZE]\n",
    "            net.zero_grad()\n",
    "            \n",
    "            out_seq = net(X)\n",
    "            first_dim, second_dim, _ = out_seq.shape\n",
    "            pred = out_seq.view(first_dim, second_dim)[:, -1]\n",
    "            loss = F.mse_loss(pred, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            \n",
    "            # Print loss every 1000 batches\n",
    "            if i % 1000 == 999:\n",
    "                avg_loss = running_loss / epoch_steps\n",
    "                print(f\"Epoch {epoch}, steps {epoch_steps-1000}:{epoch_steps} avg loss: {avg_loss}\")\n",
    "                running_loss = 0 \n",
    "                \n",
    "        # Validate each epoch\n",
    "        val_loss = 0\n",
    "        val_steps = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(val_X)-BATCH_SIZE, BATCH_SIZE):\n",
    "                X = val_X[i:i+BATCH_SIZE]\n",
    "                y = val_y[i:i+BATCH_SIZE]\n",
    "                out_seq = net(X)\n",
    "                first_dim, second_dim, _ = out_seq.shape\n",
    "                pred = out_seq.view(first_dim, second_dim)[:, -1]\n",
    "                loss = F.mse_loss(pred, y)\n",
    "                val_loss += loss.item()\n",
    "                val_steps += 1\n",
    "        \n",
    "        with tune.checkpoint_dir(step=epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "            \n",
    "        tune.report(val_loss=(val_loss/val_steps), train_loss=(running_loss/epoch_steps))\n",
    "        print(f\"Finished epoch {epoch} in {datetime.now()-epoch_start}\")\n",
    "    print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c25364f-68c6-4cca-a250-91683896c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808f64a-6856-4574-847f-5b8a686d147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config= {\n",
    "    'hidden_size': tune.sample_from(lambda spec: 2 ** np.random.randint(3,8)), # 2^3 to 2^8, 8 to 256\n",
    "    'num_lstm_layers': tune.sample_from(lambda spec: np.random.randint(1,5)),\n",
    "    'dropout': tune.sample_from(lambda spec: np.random.rand() * 0.5), # [0,0.5]\n",
    "    'learning_rate': tune.sample_from(lambda spec: 10 ** (-10 * np.random.rand())),\n",
    "    'window_size': tune.sample_from(lambda spec: 6 * np.random.randint(1,12)), # 6 Observations = 30 minutes, 6*12 = 6 hours\n",
    "    'batch_size': tune.sample_from(lambda spec: 2 ** np.random.randint(3,8)), # 2^3 to 2^8, 8 to 256\n",
    "    'epoch': tune.sample_from(lambda spec: 1 * np.random.randint(1,50))\n",
    "}\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=10,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2)\n",
    "\n",
    "reporter = JupyterNotebookReporter(\n",
    "    metric_columns = [\"val_loss\", \"loss\", \"training_iteration\"]\n",
    ")\n",
    "\n",
    "result = tune.run(\n",
    "    partial(train_lstm),\n",
    "    resources_per_trial={\"cpu\":1},\n",
    "    config=config,\n",
    "    num_samples=3,\n",
    "    progress_reporter=reporter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2321b45a-da30-4965-bd42-82ca1dbe04b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = result.get_best_trial('val_loss', 'min', 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34307f35-51a7-4609-b932-35bcd706102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final validation loss: {}\".format(\n",
    "    best_trial.last_result[\"val_loss\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry_kernel",
   "language": "python",
   "name": "poetry_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
