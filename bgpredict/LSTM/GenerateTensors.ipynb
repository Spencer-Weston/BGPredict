{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eef16346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import s3fs\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functools import partial\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ab5609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing Functions\n",
    "def load_data_to_df():\n",
    "    location = f\"postgresql://postgres:{os.environ.get('db_password')}@{os.environ.get('db_location')}\"\n",
    "    engine = create_engine(location)\n",
    "    conn = engine.connect()\n",
    "    raw_df = pd.read_sql(\"select * from public.vw_final_dataset\", conn)\n",
    "    return raw_df\n",
    "\n",
    "def clean_data(df):\n",
    "    # Drop rows with no Y value\n",
    "    df = df.dropna(subset='bg')\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values(by=\"timestamp_clean\")\n",
    "    \n",
    "    # Set index to time_stamp_clean\n",
    "    df.index = df['timestamp_clean']\n",
    "    df = df.drop(labels=['timestamp_clean'], axis=1)\n",
    "    \n",
    "    # Drop first row by subject which has data quality issues\n",
    "    df = df[df.groupby('subjectid').cumcount() > 0] \n",
    "    \n",
    "    # Drop columns that are indices, irrelevant, or capture in OHE variables\n",
    "    drop_cols = ['subjectid', 'entryid', 'timestamp', 'date', 'time']\n",
    "    df = df.drop(labels=drop_cols, axis=1)\n",
    "    \n",
    "    # Drop null week days (In need of better solution)\n",
    "    df = df.loc[~df['weekday'].isna(), :]\n",
    "    \n",
    "    # Fill nulls (lag BG values) with 0 to indicate data is unavailable\n",
    "    print(f\"Null values to be filled by column:\")\n",
    "    nulls = df.isna().sum()\n",
    "    null_idx = list(nulls.index)\n",
    "    vals = list(nulls)\n",
    "    for col, val in list(zip(null_idx, vals)):\n",
    "        if val > 0:\n",
    "            print(col,val)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    # One hot Encode Weekdays\n",
    "    weekdays = np.unique(df['weekday'])\n",
    "    ohe_weekdays = [f\"ohe_{day}\" for day in weekdays]\n",
    "    df[ohe_weekdays] = pd.get_dummies(df.weekday)\n",
    "    df = df.drop(labels=\"weekday\", axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def split_and_scale(df, scaler=None):\n",
    "    # train/val/test split\n",
    "    train_df = df.loc[df['train_set'] ==1, :]\n",
    "    val_df = df.loc[df['validation_set'] ==1, :]\n",
    "    test_df = df.loc[df['test_set'] == 1, :] \n",
    "    \n",
    "    # Extract y vars\n",
    "    train_y = train_df['bg']\n",
    "    val_y = val_df['bg']\n",
    "    test_y = test_df['bg']\n",
    "    \n",
    "    # Drop non-X columns\n",
    "    drop_cols = ['train_set', 'validation_set', 'test_set', 'bg']\n",
    "    train_df = train_df.drop(labels=drop_cols, axis=1)\n",
    "    val_df = val_df.drop(labels=drop_cols, axis=1)\n",
    "    test_df = test_df.drop(labels=drop_cols, axis=1)\n",
    "    \n",
    "    # Select Scaling columns (i.e. don't scale one hot encoded variables)\n",
    "    ohe_cols = train_df.columns[train_df.columns.str.contains('ohe')]\n",
    "    scaling_cols = train_df.columns.difference(ohe_cols)\n",
    "    print(f\"{len(ohe_cols)} one hot encoded columns \")\n",
    "    print(f\"{len(scaling_cols)} scaled columns\")\n",
    "    \n",
    "    # Fit Scaler if one isn't provided \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_df[scaling_cols])\n",
    "    \n",
    "    # Perform Scaling \n",
    "    train_array = scaler.transform(train_df[scaling_cols])\n",
    "    val_array = scaler.transform(val_df[scaling_cols])\n",
    "    test_array = scaler.transform(test_df[scaling_cols])\n",
    "    \n",
    "    # Recombine Scaled Data into DataFrame Format \n",
    "    train_scaled_df = pd.DataFrame(train_array, columns=scaling_cols, index=train_df.index)\n",
    "    val_scaled_df = pd.DataFrame(val_array, columns=scaling_cols, index=val_df.index)\n",
    "    test_scaled_df = pd.DataFrame(test_array, columns=scaling_cols, index=test_df.index)\n",
    "    \n",
    "    train_df = pd.concat([train_scaled_df, train_df.loc[:,ohe_cols], train_y], axis=1)\n",
    "    val_df = pd.concat([val_scaled_df, val_df.loc[:,ohe_cols], val_y], axis=1)\n",
    "    test_df = pd.concat([test_scaled_df, test_df.loc[:,ohe_cols], test_y], axis=1)\n",
    "    \n",
    "    return train_df, val_df, test_df, scaler\n",
    "\n",
    "def split_and_scale_holdouts(df, scaler):\n",
    "    test_y = df['bg']\n",
    "    drop_cols = ['train_set', 'validation_set', 'test_set', 'bg']\n",
    "    X = df.drop(drop_cols, axis=1)\n",
    "    \n",
    "    # Select Scaling columns (i.e. don't scale one hot encoded variables)\n",
    "    ohe_cols = X.columns[X.columns.str.contains('ohe')]\n",
    "    scaling_cols = X.columns.difference(ohe_cols)\n",
    "    print(f\"{len(ohe_cols)} one hot encoded columns \")\n",
    "    print(f\"{len(scaling_cols)} scaled columns\")\n",
    "    \n",
    "    X_array = scaler.transform(X[scaling_cols])\n",
    "    \n",
    "    X_scaled = pd.DataFrame(X_array, columns=scaling_cols, index=X.index)\n",
    "    test_df = pd.concat([X_scaled, X.loc[:,ohe_cols], test_y], axis=1)\n",
    "    return test_df\n",
    "\n",
    "def df_to_Xy_tensors(df, window_size=12):\n",
    "    X = []\n",
    "    y = []\n",
    "    num_features = len(df.columns) - 1\n",
    "    for idx in tqdm(range(window_size, len(df)-window_size)):\n",
    "        window_df = df.iloc[idx-window_size:idx]\n",
    "        X.append(window_df.loc[:, df.columns != 'bg'].values)\n",
    "        # The first element is the y value associated with the sequence of X values \n",
    "        y.append(window_df['bg'].iloc[0])\n",
    "        \n",
    "    X_tensor = torch.cat([torch.tensor(i).float() for i in X]).view(len(X), window_size, num_features)\n",
    "    y_tensor = torch.tensor(y).float()\n",
    "    return X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a6ed82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31495f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "held_out_subjects = [60844515, 41131654, 40997757, 94200862, 91161972, 28608066,\n",
    "                     76817975, 37875431, 63047517, 72492570, 80796147, 87770486,\n",
    "                     95851255, 70454270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61a52fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in 0:09:03.228632\n"
     ]
    }
   ],
   "source": [
    "# Configure data\n",
    "# Load data and remove holdout subjects\n",
    "start = datetime.now()\n",
    "raw_df = load_data_to_df()\n",
    "print(f\"Data loaded in {datetime.now() - start}\")\n",
    "holdout_subjects = raw_df.loc[raw_df['subjectid'].isin(held_out_subjects)]\n",
    "train_subjects = raw_df.loc[~raw_df['subjectid'].isin(held_out_subjects)]\n",
    "del raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e547841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values to be filled by column:\n",
      "bg_lag_1 32768\n",
      "bg_lag_2 33941\n",
      "bg_lag_3 34198\n",
      "bg_lag_4 34729\n",
      "bg_lag_5 35258\n",
      "bg_lag_6 35452\n",
      "bg_lag_7 35867\n",
      "bg_lag_8 35992\n",
      "bg_lag_9 36207\n",
      "bg_lag_10 36429\n",
      "bg_lag_11 36641\n",
      "bg_lag_12 36773\n",
      "148 one hot encoded columns \n",
      "23 scaled columns\n"
     ]
    }
   ],
   "source": [
    "# Clean In sample Data\n",
    "clean_insample_df = clean_data(train_subjects)\n",
    "\n",
    "# Split and Scale Data\n",
    "train_df, val_df, test_df, scaler = split_and_scale(clean_insample_df)\n",
    "# holdout_scaled = split_and_scale_holdouts(clean_holdout_df, scaler)\n",
    "\n",
    "window_size = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2fb5a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e054044d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Train Tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2822590/2822590 [21:34<00:00, 2180.90it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Train Tensors\")\n",
    "train_X, train_y = df_to_Xy_tensors(train_df, window_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8352c1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = s3fs.S3FileSystem()\n",
    "# with s3.open(\"s3://bgpredict/models/lstm/tensors/train_X\", 'wb') as f:\n",
    "#     torch.save(train_X, f)\n",
    "# with s3.open(\"s3://bgpredict/models/lstm/tensors/train_y\", 'wb') as f:\n",
    "#     torch.save(train_y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "829c5636",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m train_X\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m train_y\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "del train_X\n",
    "del train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb173622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Validation Tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1346613/1346613 [10:21<00:00, 2167.06it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Validation Tensors\")\n",
    "val_X, val_y = df_to_Xy_tensors(val_df, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca98b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem()\n",
    "with s3.open(\"s3://bgpredict/models/lstm/tensors/val_X\", 'wb') as f:\n",
    "    torch.save(val_X, f)\n",
    "with s3.open(\"s3://bgpredict/models/lstm/tensors/val_y\", 'wb') as f:\n",
    "    torch.save(val_y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a241e595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Test Tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1171256/1171256 [09:03<00:00, 2153.72it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Test Tensors\")\n",
    "test_X, test_y = df_to_Xy_tensors(test_df, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3c99f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem()\n",
    "with s3.open(\"s3://bgpredict/models/lstm/tensors/test_X\", 'wb') as f:\n",
    "    torch.save(test_X, f)\n",
    "with s3.open(\"s3://bgpredict/models/lstm/tensors/test_y\", 'wb') as f:\n",
    "    torch.save(test_y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adfc4733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values to be filled by column:\n",
      "bg_lag_1 8547\n",
      "bg_lag_2 8022\n",
      "bg_lag_3 7857\n",
      "bg_lag_4 8150\n",
      "bg_lag_5 8218\n",
      "bg_lag_6 8163\n",
      "bg_lag_7 8230\n",
      "bg_lag_8 8394\n",
      "bg_lag_9 8295\n",
      "bg_lag_10 8384\n",
      "bg_lag_11 8460\n",
      "bg_lag_12 8495\n",
      "148 one hot encoded columns \n",
      "23 scaled columns\n"
     ]
    }
   ],
   "source": [
    "clean_holdouts = clean_data(holdout_subjects)\n",
    "holdsout_df = split_and_scale_holdouts(clean_holdouts, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "458c9cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Holdout Tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 572731/572731 [04:24<00:00, 2166.76it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Holdout Tensors\")\n",
    "holdout_X, holdout_y = df_to_Xy_tensors(holdsout_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a84ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem()\n",
    "with s3.open(\"s3://bgpredict/models/lstm/tensors/holdout_X\", 'wb') as f:\n",
    "    torch.save(holdout_X, f)\n",
    "with s3.open(\"s3://bgpredict/models/lstm/tensors/holdout_y\", 'wb') as f:\n",
    "    torch.save(holdout_y, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
