{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef16346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import s3fs\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functools import partial\n",
    "import ray\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ab5609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing Functions\n",
    "def load_data_to_df():\n",
    "    location = f\"postgresql://postgres:{os.environ.get('db_password')}@{os.environ.get('db_location')}\"\n",
    "    engine = create_engine(location)\n",
    "    conn = engine.connect()\n",
    "    columns = pd.read_sql(\"select column_name from information_schema.columns where table_schema = 'public' and table_name = 'tb_final_dataset'\", conn)\n",
    "    columns = list(raw_df['column_name'])\n",
    "    select_cols = [c for c in columns if not \"ohe\" in c]\n",
    "    s= \", \"\n",
    "    string = s.join(select_cols)\n",
    "    raw_df = pd.read_sql(f\"select {string} from public.vw_final_dataset\", conn)\n",
    "    return raw_df\n",
    "\n",
    "def clean_data(df):\n",
    "    # Drop rows with no Y value\n",
    "    df = df.dropna(subset='bg')\n",
    "    \n",
    "    # Drop OHE columns\n",
    "    df = df.loc[:, ~df.columns.str.contains(\"ohe\")]\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values(by=\"timestamp_clean\")\n",
    "    \n",
    "    # Set index to time_stamp_clean\n",
    "    df.index = df['timestamp_clean']\n",
    "    df = df.drop(labels=['timestamp_clean'], axis=1)\n",
    "    \n",
    "    # Drop first row by subject which has data quality issues\n",
    "    df = df[df.groupby('subjectid').cumcount() > 0] \n",
    "    \n",
    "    # Drop columns that are indices, irrelevant, or capture in OHE variables\n",
    "    drop_cols = ['subjectid', 'entryid', 'timestamp', 'date', 'time']\n",
    "    df = df.drop(labels=drop_cols, axis=1)\n",
    "    \n",
    "    # Drop null week days (In need of better solution)\n",
    "    df = df.loc[~df['weekday'].isna(), :]\n",
    "    \n",
    "    # Fill nulls (lag BG values) with 0 to indicate data is unavailable\n",
    "    print(f\"Null values to be filled by column:\")\n",
    "    nulls = df.isna().sum()\n",
    "    null_idx = list(nulls.index)\n",
    "    vals = list(nulls)\n",
    "    for col, val in list(zip(null_idx, vals)):\n",
    "        if val > 0:\n",
    "            print(col,val)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    # One hot Encode Weekdays\n",
    "    weekdays = np.unique(df['weekday'])\n",
    "    ohe_weekdays = [f\"ohe_{day}\" for day in weekdays]\n",
    "    df[ohe_weekdays] = pd.get_dummies(df.weekday)\n",
    "    df = df.drop(labels=\"weekday\", axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def split_and_scale(df, scaler=None):\n",
    "    # train/val/test split\n",
    "    train_df = df.loc[df['train_set'] ==1, :]\n",
    "    val_df = df.loc[df['validation_set'] ==1, :]\n",
    "    test_df = df.loc[df['test_set'] == 1, :] \n",
    "    \n",
    "    # Extract y vars\n",
    "    train_y = train_df['bg']\n",
    "    val_y = val_df['bg']\n",
    "    test_y = test_df['bg']\n",
    "    \n",
    "    # Drop non-X columns\n",
    "    drop_cols = ['train_set', 'validation_set', 'test_set', 'bg']\n",
    "    train_df = train_df.drop(labels=drop_cols, axis=1)\n",
    "    val_df = val_df.drop(labels=drop_cols, axis=1)\n",
    "    test_df = test_df.drop(labels=drop_cols, axis=1)\n",
    "    \n",
    "    # Select Scaling columns (i.e. don't scale one hot encoded variables)\n",
    "    ohe_cols = train_df.columns[train_df.columns.str.contains('ohe')]\n",
    "    scaling_cols = train_df.columns.difference(ohe_cols)\n",
    "    print(f\"{len(ohe_cols)} one hot encoded columns \")\n",
    "    print(f\"{len(scaling_cols)} scaled columns\")\n",
    "    \n",
    "    # Fit Scaler if one isn't provided \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_df[scaling_cols])\n",
    "    \n",
    "    # Perform Scaling \n",
    "    train_array = scaler.transform(train_df[scaling_cols])\n",
    "    val_array = scaler.transform(val_df[scaling_cols])\n",
    "    test_array = scaler.transform(test_df[scaling_cols])\n",
    "    \n",
    "    # Recombine Scaled Data into DataFrame Format \n",
    "    train_scaled_df = pd.DataFrame(train_array, columns=scaling_cols, index=train_df.index)\n",
    "    val_scaled_df = pd.DataFrame(val_array, columns=scaling_cols, index=val_df.index)\n",
    "    test_scaled_df = pd.DataFrame(test_array, columns=scaling_cols, index=test_df.index)\n",
    "    \n",
    "    train_df = pd.concat([train_scaled_df, train_df.loc[:,ohe_cols], train_y], axis=1)\n",
    "    val_df = pd.concat([val_scaled_df, val_df.loc[:,ohe_cols], val_y], axis=1)\n",
    "    test_df = pd.concat([test_scaled_df, test_df.loc[:,ohe_cols], test_y], axis=1)\n",
    "    \n",
    "    return train_df, val_df, test_df, scaler\n",
    "\n",
    "def split_and_scale_holdouts(df, scaler):\n",
    "    test_y = df['bg']\n",
    "    drop_cols = ['train_set', 'validation_set', 'test_set', 'bg']\n",
    "    X = df.drop(drop_cols, axis=1)\n",
    "    \n",
    "    # Select Scaling columns (i.e. don't scale one hot encoded variables)\n",
    "    ohe_cols = X.columns[X.columns.str.contains('ohe')]\n",
    "    scaling_cols = X.columns.difference(ohe_cols)\n",
    "    print(f\"{len(ohe_cols)} one hot encoded columns \")\n",
    "    print(f\"{len(scaling_cols)} scaled columns\")\n",
    "    \n",
    "    X_array = scaler.transform(X[scaling_cols])\n",
    "    \n",
    "    X_scaled = pd.DataFrame(X_array, columns=scaling_cols, index=X.index)\n",
    "    test_df = pd.concat([X_scaled, X.loc[:,ohe_cols], test_y], axis=1)\n",
    "    return test_df\n",
    "\n",
    "def df_to_Xy_tensors(df, window_size=12):\n",
    "    X = []\n",
    "    y = []\n",
    "    num_features = len(df.columns) - 1\n",
    "    for idx in tqdm(range(window_size, len(df)-window_size)):\n",
    "        window_df = df.iloc[idx-window_size:idx]\n",
    "        X.append(window_df.loc[:, df.columns != 'bg'].values)\n",
    "        # The first element is the y value associated with the sequence of X values \n",
    "        y.append(window_df['bg'].iloc[0])\n",
    "        \n",
    "    X_tensor = torch.cat([torch.tensor(i).float() for i in X]).view(len(X), window_size, num_features)\n",
    "    y_tensor = torch.tensor(y).float()\n",
    "    return X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a6ed82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31495f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "held_out_subjects = [60844515, 41131654, 40997757, 94200862, 91161972, 28608066,\n",
    "                     76817975, 37875431, 63047517, 72492570, 80796147, 87770486,\n",
    "                     95851255, 70454270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61a52fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in 0:00:00.000039\n"
     ]
    }
   ],
   "source": [
    "# Configure data\n",
    "# Load data and remove holdout subjects\n",
    "start = datetime.now()\n",
    "raw_df = load_data_to_df()\n",
    "print(f\"Data loaded in {datetime.now() - start}\")\n",
    "holdout_subjects = raw_df.loc[raw_df['subjectid'].isin(held_out_subjects)]\n",
    "train_subjects = raw_df.loc[~raw_df['subjectid'].isin(held_out_subjects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e547841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values to be filled by column:\n",
      "bg_lag_1 32768\n",
      "bg_lag_2 33941\n",
      "bg_lag_3 34198\n",
      "bg_lag_4 34729\n",
      "bg_lag_5 35258\n",
      "bg_lag_6 35452\n",
      "bg_lag_7 35867\n",
      "bg_lag_8 35992\n",
      "bg_lag_9 36207\n",
      "bg_lag_10 36429\n",
      "bg_lag_11 36641\n",
      "bg_lag_12 36773\n",
      "7 one hot encoded columns \n",
      "23 scaled columns\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clean_holdout_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Split and Scale Data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train_df, val_df, test_df, scaler \u001b[38;5;241m=\u001b[39m split_and_scale(clean_insample_df)\n\u001b[0;32m----> 6\u001b[0m holdout_scaled \u001b[38;5;241m=\u001b[39m split_and_scale_holdouts(\u001b[43mclean_holdout_df\u001b[49m, scaler)\n\u001b[1;32m      8\u001b[0m window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_holdout_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Clean In sample Data\n",
    "clean_insample_df = clean_data(train_subjects)\n",
    "# Split and Scale Data\n",
    "train_df, val_df, test_df, scaler = split_and_scale(clean_insample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7292d9d5-20c8-48dc-9cf1-8243fe8e029d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values to be filled by column:\n",
      "bg_lag_1 8547\n",
      "bg_lag_2 8022\n",
      "bg_lag_3 7857\n",
      "bg_lag_4 8150\n",
      "bg_lag_5 8218\n",
      "bg_lag_6 8163\n",
      "bg_lag_7 8230\n",
      "bg_lag_8 8394\n",
      "bg_lag_9 8295\n",
      "bg_lag_10 8384\n",
      "bg_lag_11 8460\n",
      "bg_lag_12 8495\n",
      "7 one hot encoded columns \n",
      "23 scaled columns\n"
     ]
    }
   ],
   "source": [
    "clean_holdout_df = clean_data(holdout_subjects)\n",
    "holdout_scaled = split_and_scale_holdouts(clean_holdout_df, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2fb5a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e054044d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Train Tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2822590/2822590 [21:39<00:00, 2172.30it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Train Tensors\")\n",
    "train_X, train_y = df_to_Xy_tensors(train_df, window_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8352c1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem()\n",
    "with s3.open(\"s3://bgpredict/models/lstm/tensors/train_X.pt\", 'wb') as f:\n",
    "    torch.save(train_X, f)\n",
    "with s3.open(\"s3://bgpredict/models/lstm/tensors/train_y.pt\", 'wb') as f:\n",
    "    torch.save(train_y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "622f9e7a-08eb-4de6-bdea-c810db588516",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./train_X.pt\", 'wb') as f:\n",
    "    torch.save(train_X, f)\n",
    "with open(\"./train_y.pt\", 'wb') as f:\n",
    "    torch.save(train_y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "829c5636",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m train_X\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m train_y\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "del train_X\n",
    "del train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb173622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Validation Tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1346613/1346613 [10:09<00:00, 2209.42it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Validation Tensors\")\n",
    "val_X, val_y = df_to_Xy_tensors(val_df, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca98b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem()\n",
    "with s3.open(\"s3://bgpredict/models/lstm/tensors/val_X.pt\", 'wb') as f:\n",
    "    torch.save(val_X, f)\n",
    "with s3.open(\"s3://bgpredict/models/lstm/tensors/val_y.pt\", 'wb') as f:\n",
    "    torch.save(val_y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac8e40de-53e4-4c80-bf91-8f5005c45d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./val_X.pt\", 'wb') as f:\n",
    "    torch.save(val_X, f)\n",
    "with open(\"./val_y.pt\", 'wb') as f:\n",
    "    torch.save(val_y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a241e595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Test Tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1171256/1171256 [08:46<00:00, 2226.30it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Test Tensors\")\n",
    "test_X, test_y = df_to_Xy_tensors(test_df, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3c99f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem()\n",
    "with s3.open(\"s3://bgpredict/models/lstm/tensors/test_X.pt\", 'wb') as f:\n",
    "    torch.save(test_X, f)\n",
    "with s3.open(\"s3://bgpredict/models/lstm/tensors/test_y.pt\", 'wb') as f:\n",
    "    torch.save(test_y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d98c092-bcc4-4639-ae08-2ea42d1e31c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./test_X.pt\", 'wb') as f:\n",
    "    torch.save(test_X, f)\n",
    "with open(\"./test_y.pt\", 'wb') as f:\n",
    "    torch.save(test_y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "458c9cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Holdout Tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 572731/572731 [04:20<00:00, 2202.44it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Holdout Tensors\")\n",
    "holdout_X, holdout_y = df_to_Xy_tensors(holdout_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a84ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem()\n",
    "with s3.open(\"s3://bgpredict/models/lstm/tensors/holdout_X.pt\", 'wb') as f:\n",
    "    torch.save(holdout_X, f)\n",
    "with s3.open(\"s3://bgpredict/models/lstm/tensors/holdout_y.pt\", 'wb') as f:\n",
    "    torch.save(holdout_y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0adca2ea-92db-436a-bc55-58c6866a7d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./holdout_X.pt\", 'wb') as f:\n",
    "    torch.save(holdout_X, f)\n",
    "with open(\"./holdout_y.pt\", 'wb') as f:\n",
    "    torch.save(holdout_y, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
