{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ca1ade-27df-437b-b5f2-ce8f7dfb1848",
   "metadata": {},
   "source": [
    "# Aggregate Night Scout Statistics\n",
    "Author: Spencer Weston\n",
    "\n",
    "This notebook exists to begin evaluating the nightscout/openaps data as a whole. This notebook should help determine which variables we can drop and generate some metadata. We also need to validate some assumptions across all subjects. We'll get setup by accumulating all the file pathes into a single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "079306b6-4215-497c-9695-ddd677708927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re \n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pytz\n",
    "from collections import namedtuple\n",
    "from sortedcontainers import SortedDict\n",
    "from pandas.errors import ParserError\n",
    "from pandas.errors import OutOfBoundsDatetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ec289c8-7ec3-41f3-8eb5-9cd50a3ac973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\spenc\\\\Documents\\\\Berkeley\\\\Capstone\\\\BGPredict\\\\Notebooks'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58e287e5-2502-4ed4-9c47-2bc98fb2478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SubjectInfo = namedtuple(\"SubjectInfo\", ['path', 'subject_id', 'devicestatus', 'entries', 'treatments'])\n",
    "\n",
    "def generate_subject_info(data_dir, subject_number):\n",
    "    direct_sharing_folder = 'direct-sharing-31'\n",
    "    path = f\"{data_dir}/{subject_number}\"\n",
    "    # Some individuals have multiple direct-sharing folders. Ensure the correct folder exists and add it to path\n",
    "    # For example, one individual has a folder with menstruation data\n",
    "    if direct_sharing_folder not in os.listdir(path):\n",
    "        return None\n",
    "    else:\n",
    "        path = f\"{path}/{direct_sharing_folder}\"\n",
    "    \n",
    "    subject_dirs = [folder for folder in os.listdir(path)]\n",
    "    relevant_dir_names = [\"treatments\", \"devicestatus\", \"entries\"]\n",
    "    relevant_dirs = {k: [] for k in relevant_dir_names}\n",
    "    # Identify every file associated with treatments, device status, or entries and store them in a dictionary\n",
    "    for folder in subject_dirs:\n",
    "        for dir_name in relevant_dir_names:\n",
    "            if dir_name in folder and folder.endswith(\"_csv\"):\n",
    "                dir_path = f\"{path}/{folder}\"\n",
    "                files = [f\"{dir_path}/{file}\" for file in os.listdir(dir_path)]\n",
    "                relevant_dirs[dir_name].extend(files)\n",
    "    return SubjectInfo(path=path, subject_id=subject_number, devicestatus=relevant_dirs['devicestatus'],\n",
    "                      entries=relevant_dirs['entries'], treatments=relevant_dirs['treatments'])\n",
    "    \n",
    "# subject = SubjectInfo()\n",
    "data_dir = \"C:\\\\Users\\spenc\\Documents\\Berkeley\\Capstone\\n=183_OpenAPS_Data_Commons_August_2021_UNZIPPED\"\n",
    "data_dir = data_dir.replace(\"\\\\\", \"/\")\n",
    "data_dir = data_dir.replace(\"\\n\", \"/n\")\n",
    "# isnumeric validates that the folder comes from openaps/nightscout\n",
    "subject_dirs = [generate_subject_info(data_dir, folder) for folder in os.listdir(data_dir) if folder.isnumeric()] \n",
    "\n",
    "# None will be returned if an individual has no  \n",
    "try:\n",
    "    while subject_dirs.index(None):\n",
    "        idx = subject_dirs.index(None)\n",
    "        subject_dirs.pop(idx)\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1453f5ab-7967-48e0-af0c-071b84cf6183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45380, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>bg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-08-04T23:58:50Z</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-08-04T23:53:51Z</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-08-04T23:48:51Z</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-08-04T23:43:51Z</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-08-04T23:38:51Z</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   time    bg\n",
       "0  2018-08-04T23:58:50Z   150\n",
       "1  2018-08-04T23:53:51Z   153\n",
       "2  2018-08-04T23:48:51Z   155\n",
       "3  2018-08-04T23:43:51Z   159\n",
       "4  2018-08-04T23:38:51Z   164"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [i for i in subject_dirs if i.subject_id==\"00221634\"]\n",
    "file = s[0].entries[0]\n",
    "df = pd.read_csv(file, header=None, low_memory=False)\n",
    "df.columns = ['time', 'bg']\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72e09b62-6f78-4202-ae6b-38d8f392d383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time,bg\n",
      "2018-08-04T23:58:50Z, 150\n",
      "2018-08-04T23:53:51Z, 153\n",
      "2018-08-04T23:48:51Z, 155\n",
      "2018-08-04T23:43:51Z, 159\n",
      "2018-08-04T23:38:51Z, 164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = df.head().to_csv(index=False)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "32f8e4ca-6887-466f-a52f-fd1091bf456a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45264, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>bg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-08-04T23:58:50Z</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-08-04T23:53:51Z</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-08-04T23:48:51Z</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-08-04T23:43:51Z</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-08-04T23:38:51Z</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   time    bg\n",
       "0  2018-08-04T23:58:50Z   150\n",
       "1  2018-08-04T23:53:51Z   153\n",
       "2  2018-08-04T23:48:51Z   155\n",
       "3  2018-08-04T23:43:51Z   159\n",
       "4  2018-08-04T23:38:51Z   164"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file, header=None, low_memory=False)\n",
    "df.columns = ['time', 'bg']\n",
    "print(df.shape)\n",
    "df.head()\n",
    "df = df.loc[df.bg!=\" null\", :]\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7249130-d7fe-4c85-8091-ae9233be1ddc",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "Create some rough summary statistics based on the available files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b442747-4f88-48a2-9f0c-6814c78805f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_presence = {\"treatments\": [], \"devicestatus\": [], \"entries\": []}\n",
    "for x in subject_dirs:\n",
    "    for key in file_presence.keys():\n",
    "        tuple_idx = x._fields.index(key)\n",
    "        files = x[tuple_idx]\n",
    "        file_presence[key].append(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb68e40-0fb7-414c-9abf-63619cbeaca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['treatments', 'devicestatus', 'entries'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_presence.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024be9fe-8635-4127-a5f5-cd953c4c405f",
   "metadata": {},
   "source": [
    "Here, we look at subjects with no device status but treatment data and subjects with no folders associated with their data. If they have no folders, that's an easy exclusion criteria. If they have treatment but not device status data, we'll need to validate rather the treatment data appears equivalently formatted to the device status from other olders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12cf784b-e463-4b38-a229-66ef1832e5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDT: ['42052178', '50311906', '61179686', '66773091'] \n",
      " \n",
      " NF: ['32635618', '51359431'] \n",
      " \n",
      " NE: []\n"
     ]
    }
   ],
   "source": [
    "# Get count of the number of files for each subject\n",
    "subjects = [x.subject_id for x in subject_dirs]\n",
    "file_count = list(zip(subjects, *[file_presence[k] for k in file_presence.keys()]))\n",
    "\n",
    "# subjects with no device status but treatment data; will need to be evaluated for data integrity\n",
    "no_device_but_treatment = [] \n",
    "# Subjects with no folders associated with their data; exclusion criterion\n",
    "no_folders = [] \n",
    "# No blood glucose data; exclusion criterion \n",
    "no_entries = []\n",
    "\n",
    "for x in file_count:\n",
    "    # no entries\n",
    "    if x[1] < 1:\n",
    "        if x[2] == 0 and x[3] == 0:\n",
    "            no_folders.append(x[0])\n",
    "        else: \n",
    "            no_entries.append(x[0])\n",
    "        next \n",
    "    # no device status but treatment \n",
    "    if x[2] == 0 and x[3] > 0:\n",
    "        no_device_but_treatment.append(x[0])\n",
    "        next\n",
    "\n",
    "print(f\"NDT: {no_device_but_treatment} \\n \\n NF: {no_folders} \\n \\n NE: {no_entries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df8ede-7897-411e-b764-e1a8d8ad0b32",
   "metadata": {},
   "source": [
    "### Evaluate Subjects with No Device Status but Treatment Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "637412c0-947e-457d-852d-698242556913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from NightScoutJoinAnalysis\n",
    "def define_column_superset(dataframes: list):\n",
    "    superset = set()\n",
    "    for df in dataframes:\n",
    "        cols = list(df.columns)\n",
    "        superset = superset.union(cols)\n",
    "    return superset\n",
    "\n",
    "def apply_superset(df, superset):\n",
    "    df_cols = set(list(df.columns))\n",
    "    set_diff = superset.difference(df_cols)\n",
    "    n = len(df)\n",
    "    additional_col_df = pd.DataFrame({k: [None for _ in range(n)] for k in set_diff})\n",
    "    new_df = pd.concat([df, additional_col_df], axis=1)\n",
    "    return new_df\n",
    "\n",
    "def concat_dfs(dataframes: list):\n",
    "    return pd.concat(dataframes, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23892304-fdeb-4dac-ad93-e84478fcd8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: 42052178, Num_files: 6\n",
      "(15000, 53)\n",
      "(15000, 56)\n",
      "(15000, 57)\n",
      "(15000, 61)\n",
      "(15000, 54)\n",
      "(5911, 348)\n",
      "Subject: 50311906, Num_files: 3\n",
      "(205, 48)\n",
      "(15000, 88)\n",
      "(10835, 81)\n",
      "Subject: 61179686, Num_files: 5\n",
      "(15000, 258)\n",
      "(15000, 16)\n",
      "(15000, 16)\n",
      "(15000, 16)\n",
      "(1272, 18)\n",
      "Subject: 66773091, Num_files: 6\n",
      "(15000, 16)\n",
      "(15000, 16)\n",
      "(15000, 16)\n",
      "(15000, 16)\n",
      "(15000, 16)\n",
      "(10132, 16)\n"
     ]
    }
   ],
   "source": [
    "ndt_subjects = [x for x in subject_dirs if x.subject_id in no_device_but_treatment]\n",
    "ndt_dfs = {}\n",
    "for subj in ndt_subjects:\n",
    "    print(f\"Subject: {subj.subject_id}, Num_files: {len(subj.treatments)}\")\n",
    "    ndt_dfs.update({subj.subject_id: [pd.read_csv(file, dtype=str) for file in subj.treatments]}) \n",
    "    for df in ndt_dfs[subj.subject_id]:\n",
    "        print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "252d6504-e2fa-4980-9c53-eb27bc0bea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list = [df for df in ndt_dfs[\"42052178\"]]\n",
    "# superset = define_column_superset(df_list)\n",
    "# print('supersetting')\n",
    "# superset_dfs = [apply_superset(df, superset) for df in df_list]\n",
    "# print('concatting')\n",
    "# union_df = concat_dfs(superset_dfs)\n",
    "# # union_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d97b3245-ae73-4993-b1fc-bad60f2bfc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unioned in: 0:00:01.662886\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>absolute</th>\n",
       "      <th>carbs</th>\n",
       "      <th>_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>NSCLIENT_ID</th>\n",
       "      <th>rate</th>\n",
       "      <th>eventType</th>\n",
       "      <th>enteredBy</th>\n",
       "      <th>insulin</th>\n",
       "      <th>...</th>\n",
       "      <th>raw_duration/_type</th>\n",
       "      <th>bolus/appended/0/data/5/age</th>\n",
       "      <th>square/appended/0/data/0/amount</th>\n",
       "      <th>changed/insulin_sensitivies/3/_offset</th>\n",
       "      <th>raw_rate/appended/0/data/1/amount</th>\n",
       "      <th>preBolus</th>\n",
       "      <th>changed/insulin_sensitivies/7/offset</th>\n",
       "      <th>stale/insulin_sensitivies/0/_offset</th>\n",
       "      <th>raw_rate/_body</th>\n",
       "      <th>stale/insulin_sensitivies/7/sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-10T23:40:47Z</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5cd69da3131daf5594f593e1</td>\n",
       "      <td>120</td>\n",
       "      <td>1557531647994</td>\n",
       "      <td>0</td>\n",
       "      <td>Temp Basal</td>\n",
       "      <td>S6MIS6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-10T23:39:03Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5cd69da3131daf5594f593e3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1557531661775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Correction Bolus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-10T23:30:53Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5cd69da3131daf5594f593e2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1557531053916</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Temp Basal</td>\n",
       "      <td>S6MIS6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-10T23:29:01Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5cd69da3131daf5594f593e4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1557531068046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Correction Bolus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-10T23:17:56Z</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5cd69d5a131daf5594f593cc</td>\n",
       "      <td>82</td>\n",
       "      <td>1557530276855</td>\n",
       "      <td>0</td>\n",
       "      <td>Temp Basal</td>\n",
       "      <td>S6MIS6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 353 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             created_at absolute carbs                       _id duration  \\\n",
       "0  2019-05-10T23:40:47Z        0   NaN  5cd69da3131daf5594f593e1      120   \n",
       "1  2019-05-10T23:39:03Z      NaN   NaN  5cd69da3131daf5594f593e3      NaN   \n",
       "2  2019-05-10T23:30:53Z      NaN   NaN  5cd69da3131daf5594f593e2      NaN   \n",
       "3  2019-05-10T23:29:01Z      NaN   NaN  5cd69da3131daf5594f593e4      NaN   \n",
       "4  2019-05-10T23:17:56Z        0   NaN  5cd69d5a131daf5594f593cc       82   \n",
       "\n",
       "     NSCLIENT_ID rate         eventType enteredBy insulin  ...  \\\n",
       "0  1557531647994    0        Temp Basal    S6MIS6     NaN  ...   \n",
       "1  1557531661775  NaN  Correction Bolus       NaN     0.4  ...   \n",
       "2  1557531053916  NaN        Temp Basal    S6MIS6     NaN  ...   \n",
       "3  1557531068046  NaN  Correction Bolus       NaN     0.2  ...   \n",
       "4  1557530276855    0        Temp Basal    S6MIS6     NaN  ...   \n",
       "\n",
       "  raw_duration/_type bolus/appended/0/data/5/age  \\\n",
       "0               None                        None   \n",
       "1               None                        None   \n",
       "2               None                        None   \n",
       "3               None                        None   \n",
       "4               None                        None   \n",
       "\n",
       "  square/appended/0/data/0/amount changed/insulin_sensitivies/3/_offset  \\\n",
       "0                            None                                  None   \n",
       "1                            None                                  None   \n",
       "2                            None                                  None   \n",
       "3                            None                                  None   \n",
       "4                            None                                  None   \n",
       "\n",
       "  raw_rate/appended/0/data/1/amount preBolus  \\\n",
       "0                              None     None   \n",
       "1                              None     None   \n",
       "2                              None     None   \n",
       "3                              None     None   \n",
       "4                              None     None   \n",
       "\n",
       "  changed/insulin_sensitivies/7/offset stale/insulin_sensitivies/0/_offset  \\\n",
       "0                                 None                                None   \n",
       "1                                 None                                None   \n",
       "2                                 None                                None   \n",
       "3                                 None                                None   \n",
       "4                                 None                                None   \n",
       "\n",
       "  raw_rate/_body stale/insulin_sensitivies/7/sensitivity  \n",
       "0           None                                    None  \n",
       "1           None                                    None  \n",
       "2           None                                    None  \n",
       "3           None                                    None  \n",
       "4           None                                    None  \n",
       "\n",
       "[5 rows x 353 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [df.reset_index(drop=True) for df in superset_dfs]\n",
    "start = datetime.now()\n",
    "union_df = pd.concat(test, axis = 0)\n",
    "end = datetime.now()\n",
    "print(\"Unioned in:\", end-start)\n",
    "union_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12686d-89e5-4359-8ee7-a0701a327242",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prints columns\n",
    "# cols = []\n",
    "# for i,col in enumerate(union_df.columns):\n",
    "#     cols.append(col)\n",
    "#     if i % 5 == 0:\n",
    "#         print(cols)\n",
    "#         cols = []\n",
    "union_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5abdb9-f2ba-4241-9ff1-97f587cd03cf",
   "metadata": {},
   "source": [
    "We can see that these treatment tables hold a lot of relevant information. We'd love to keep this info if possible. We'll have to do a lot of work to process the though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84347b9-ee0d-4a2a-9283-869ae8402d8d",
   "metadata": {},
   "source": [
    "## Evaluate Treatment Uniqueness\n",
    "In Nightscout join analysis, we saw that a union of treatments with duplicates dropped exactly matched a device status file. Let's see how common that is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ead9222-a725-4fb8-bb5b-edb86eb804cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subject:\n",
    "    \n",
    "    def __init__(self, subject_id, subject_path, treatment_files, device_status_files, entries_files):\n",
    "        self.subject_id = subject_id\n",
    "        self.subject_path = subject_path\n",
    "        \n",
    "        self.treatment_files = treatment_files\n",
    "        self.treatment_shapes = None\n",
    "        self.treatment_df = None\n",
    "        \n",
    "        self.device_status_files = device_status_files\n",
    "        self.device_status_shapes = None\n",
    "        self.device_status_df = None\n",
    "        \n",
    "        self.entries_files = entries_files\n",
    "        self.entries_shapes = None\n",
    "        self.entries_df = None \n",
    "        \n",
    "    \n",
    "    def get_device_status_shapes(self):\n",
    "        if self.device_status_shapes is not None:\n",
    "            return self.device_status_shapes\n",
    "        else:\n",
    "            self.device_status_shapes = [pd.read_csv(file, low_memory=False).shape for file in self.device_status_files]\n",
    "            return self.device_status_shapes\n",
    "    \n",
    "    def get_device_status_df(self):\n",
    "        if self.device_status_df is not None:\n",
    "            return self.device_status_df\n",
    "        else:\n",
    "            device_status_dfs = [pd.read_csv(file, low_memory=False).reset_index(drop=True) for file in self.device_status_files]\n",
    "            self.device_status_df = pd.concat(device_status_dfs, axis=0)\n",
    "            return self.device_status_df \n",
    "    \n",
    "    def get_treatment_shapes(self):\n",
    "        if self.treatment_shapes is not None:\n",
    "            return self.treatment_shapes\n",
    "        else:\n",
    "            self.treatment_shapes = [pd.read_csv(file, low_memory=False).shape for file in self.treatment_files]\n",
    "            return self.treatment_shapes\n",
    "        \n",
    "    def get_treatment_df(self):\n",
    "        if self.treatment_df is not None:\n",
    "            return self.treatment_df\n",
    "        else:\n",
    "            treatment_dfs = [pd.read_csv(file, low_memory=False).reset_index(drop=True) for file in self.treatment_files]\n",
    "            self.treatment_df = pd.concat(treatment_dfs, axis=0)\n",
    "            return self.treatment_df\n",
    "        \n",
    "    def get_entries_shapes(self):\n",
    "        if self.entries_shapes is not None:\n",
    "            return self.entries_shapes\n",
    "        else:\n",
    "            self.entries_shapes = [pd.read_csv(file, low_memory=False).shape for file in self.entries_files]\n",
    "            return self.entries_shapes\n",
    "    \n",
    "    def get_entries_df(self):\n",
    "        if self.entries_df is not None:\n",
    "            return self.entries_df\n",
    "        else:\n",
    "            self.entries_dfs = [pd.read_csv(file, low_memory=False, header=None).reset_index(drop=True) for file in self.entries_files]\n",
    "            return self.entries_df\n",
    "            \n",
    "    def check_equivalence(self, treatment_df=None, check_unique=False):\n",
    "        \"\"\"Check rather treatment data is a subset of or equivalent to device status data\"\"\"\n",
    "        if treatment_df is None:\n",
    "            treatment_df = self.get_treatment_df()\n",
    "        ds_df = self.get_device_status_df()\n",
    "        if treatment_df.shape == ds_df.shape:\n",
    "            treat_array = self.to_string_and_numpy(treatment_df)\n",
    "            ds_array = self.to_string_and_numpy(ds_df)\n",
    "            if np.array_equal(treat_array, ds_array):\n",
    "                print(\"Equivalent full treatment and device status\")\n",
    "                return 1\n",
    "            else:\n",
    "                print(\"Equivalent full treatment and device status shape but not elements\")\n",
    "                return 0\n",
    "        elif treatment_df.shape in self.get_device_status_shapes():\n",
    "            idx = self.get_device_status_shapes().index(treatment_df.shape)\n",
    "            ds_df = pd.read_csv(self.device_status_files[idx], low_memory=False)\n",
    "            ds_array = self.to_string_and_numpy(ds_df)\n",
    "            treat_array = self.to_string_and_numpy(treatment_df)\n",
    "            if np.array_equal(treat_array, ds_array):\n",
    "                print(\"Treatment equivalent to subset of device status\")\n",
    "                return 1 \n",
    "            else:\n",
    "                print(\"Treatment has equivalent shape to subset of device status but different elements\")\n",
    "                return 0 \n",
    "        elif not check_unique:\n",
    "            # Make a recursive check on the treatment dataframe with duplicates dropped \n",
    "            print('here')\n",
    "            self.check_equivalence(treatment_df.drop_duplicates(), check_unique=True)\n",
    "        else:\n",
    "            print(\"Treatment data is not a copy of device status data\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_string_and_numpy(df):\n",
    "        return df.as_type('str').to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc7e30-bbca-4b43-8f55-c7fd9831e0b8",
   "metadata": {},
   "source": [
    "**Subject 01352464: No duplicated treatment and device status**\n",
    "Here, we validate that subject 01352464 does not have any duplicates from the treatment data frame and that the check equivalence function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d85c413e-76aa-48ec-8f3f-134aeee05b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = subject_dirs[3]\n",
    "sub_3 = Subject(subject.subject_id, subject.path, subject.treatments, subject.devicestatus, subject.entries)\n",
    "# ds_shapes = sub_1.get_device_status_shapes()\n",
    "# treat_shapes = sub_1.get_treatment_shapes()\n",
    "treatment_df = sub_1.get_treatment_df()\n",
    "ds_df = sub_1.get_device_status_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccd5a9f7-54dd-493b-ba3e-5f5f51626057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140527, 403)\n",
      "(167799, 1222)\n"
     ]
    }
   ],
   "source": [
    "print(treatment_df.shape)\n",
    "print(ds_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae701745-73ac-43ef-aaf5-15a54adc8fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "for t_col in treatment_df.columns:\n",
    "    if t_col in ds_df.columns:\n",
    "        test.append((t_col, 1))\n",
    "    else:\n",
    "        test.append((t_col, 0))\n",
    "print(sum([x[1] for x in test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bfdce5d-ed29-4b92-8013-508d741dab31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('created_at', 1), ('_id', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [x for x in test if x[1] ==1]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4496741d-4bd5-4265-a712-dd161757c3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "no unique match\n"
     ]
    }
   ],
   "source": [
    "subject = subject_dirs[3]\n",
    "sub_3 = Subject(subject.subject_id, subject.path, subject.treatments, subject.devicestatus, subject.entries)\n",
    "sub_3.check_equivalence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7ce9dbf-2bca-426d-9c41-bbb8710a94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up variables\n",
    "del sub_3\n",
    "del treatment_df\n",
    "del ds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb10f0a-f090-4016-afdf-1340212f53ac",
   "metadata": {},
   "source": [
    "**Subject 00221634: No duplicated treatment and device status**\n",
    "In NightScoutJoinAnalysis, we saw that subject 00221634 has duplicate treatment and device status data. Evaluate rather `check_equivalence()` identifies the duplicate.\n",
    "\n",
    "Written after running the below: For reasons, it appears that this data isn't as duplicated as I found in NightScoutJoinAnalysis. So, I'm just going to full join everything. Then, we can compress data across the relavent columns to shrink the dataset horizontally. Then, we'll still have multiple rows for each blood glucose entry. We can then groupby blood glucose entry and perform aggregations on the relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de9c679d-42c7-455b-bd9a-9f86744d6604",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = [s for s in subject_dirs if s.subject_id==\"00221634\"][0]\n",
    "subject\n",
    "sub_3 = Subject(subject.subject_id, subject.path, subject.treatments, subject.devicestatus, subject.entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6248bb3-4b12-44f1-84e0-714d213a5859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/spenc/Documents/Berkeley/Capstone/n=183_OpenAPS_Data_Commons_August_2021_UNZIPPED/00221634/direct-sharing-31/00221634_treatments_2018-03-01_to_2018-08-05_csv/00221634_treatments_2018-03-01_to_2018-08-05_aa.csv', 'C:/Users/spenc/Documents/Berkeley/Capstone/n=183_OpenAPS_Data_Commons_August_2021_UNZIPPED/00221634/direct-sharing-31/00221634_treatments_2018-03-01_to_2018-08-05_csv/00221634_treatments_2018-03-01_to_2018-08-05_ab.csv']\n"
     ]
    }
   ],
   "source": [
    "print(subject.treatments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2b784e5-aa45-4fe6-92a3-ca44742de6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "Treatment data is not a copy of device status data\n"
     ]
    }
   ],
   "source": [
    "sub_3.check_equivalence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78baf880-efeb-489f-b1aa-cfae8cdb60c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26689, 59)\n",
      "(53877, 654)\n"
     ]
    }
   ],
   "source": [
    "treatment_df = sub_3.get_treatment_df()\n",
    "print(treatment_df.shape)\n",
    "ds_df = sub_3.get_device_status_df()\n",
    "print(ds_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "957992ed-7e44-41fe-bca6-cc1d27c8a74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2018-08-04 23:47:05+00:00\n",
       "1       2018-08-04 23:01:08+00:00\n",
       "2       2018-08-04 22:10:26+00:00\n",
       "3       2018-08-04 22:10:26+00:00\n",
       "4       2018-08-04 21:49:05+00:00\n",
       "                   ...           \n",
       "11684   2018-03-01 17:37:00+00:00\n",
       "11685   2018-03-01 15:01:00+00:00\n",
       "11686   2018-03-01 14:17:00+00:00\n",
       "11687   2018-03-01 12:17:51+00:00\n",
       "11688   2018-03-01 06:37:58+00:00\n",
       "Name: created_at, Length: 26689, dtype: datetime64[ns, UTC]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(treatment_df.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de4477c4-fee7-4255-bae7-d5d7855ee50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26689"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(treatment_df._id.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1bb1d-6775-4eb5-82ee-08ad8d52b4bb",
   "metadata": {},
   "source": [
    "## Verify all entries have the same number of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c177f3b5-7527-4137-8420-a470f3d5e0f5",
   "metadata": {},
   "source": [
    "This verifies all entry files have the same shape and can be naively unioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b0d3e-a134-4e48-bcbc-72c09208741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_objs = []\n",
    "for x in subject_dirs:\n",
    "    subject_objs.append(\n",
    "        Subject(x.subject_id, x.path, x.treatments, x.devicestatus, x.entries)\n",
    "    )\n",
    "for x in subject_objs:\n",
    "    shapes = x.get_entries_shapes()\n",
    "    for shape in shapes:\n",
    "        if shape[1] != 2:\n",
    "            print(f\"{x.subject_id} has malconformed shape. {shapes}\")\n",
    "            break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e414f-9736-405a-8686-5ec4b935c93a",
   "metadata": {},
   "source": [
    "## Raw join of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "81b2268c-d5ac-4b3c-90b1-8ae9a068fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subject:\n",
    "    \n",
    "    def __init__(self, subject_id, subject_path, treatment_files, device_status_files, entries_files):\n",
    "        self.subject_id = subject_id\n",
    "        self.subject_path = subject_path\n",
    "        \n",
    "        self.treatment_files = treatment_files\n",
    "        self.treatment_shapes = None\n",
    "        self.treatment_df = None\n",
    "        \n",
    "        self.device_status_files = device_status_files\n",
    "        self.device_status_shapes = None\n",
    "        self.device_status_df = None\n",
    "        \n",
    "        self.entries_files = entries_files\n",
    "        self.entries_shapes = None\n",
    "        self.entries_df = None \n",
    "        \n",
    "        self.join_table = None \n",
    "        \n",
    "    def get_device_status_shapes(self):\n",
    "        if self.device_status_shapes is not None:\n",
    "            return self.device_status_shapes\n",
    "        else:\n",
    "            self.device_status_shapes = [pd.read_csv(file, low_memory=False).shape for file in self.device_status_files]\n",
    "            return self.device_status_shapes\n",
    "    \n",
    "    def get_device_status_df(self):\n",
    "        if len(self.device_status_files) == 0:\n",
    "            return None\n",
    "        if self.device_status_df is not None:\n",
    "            return self.device_status_df\n",
    "        else:\n",
    "            device_status_dfs = [pd.read_csv(file, low_memory=False).reset_index(drop=True) for file in self.device_status_files]\n",
    "            self.device_status_df = pd.concat(device_status_dfs, axis=0)\n",
    "            self.device_status_df['timestamp'] = pd.to_datetime(self.device_status_df['created_at'])\n",
    "            self.device_status_df['devicestatusid'] = [i for i in range(len(self.device_status_df))]\n",
    "            return self.device_status_df \n",
    "    \n",
    "    def get_treatment_shapes(self):\n",
    "        if self.treatment_shapes is not None:\n",
    "            return self.treatment_shapes\n",
    "        else:\n",
    "            self.treatment_shapes = [pd.read_csv(file, low_memory=False).shape for file in self.treatment_files]\n",
    "            return self.treatment_shapes\n",
    "        \n",
    "    def get_treatment_df(self):\n",
    "        if self.treatment_df is not None:\n",
    "            return self.treatment_df\n",
    "        else:\n",
    "            treatment_dfs = [pd.read_csv(file, low_memory=False).reset_index(drop=True) for file in self.treatment_files]\n",
    "            self.treatment_df = pd.concat(treatment_dfs, axis=0)\n",
    "            try:\n",
    "                self.treatment_df['timestamp'] = pd.to_datetime(self.treatment_df[\"created_at\"])\n",
    "            except ValueError:\n",
    "                # Somewhat randomly, for unclear reasons, we receive the following error:\n",
    "                # ValueError: cannot reindex from a duplicate axis\n",
    "                # Resetting index seems to resolve this.\n",
    "                self.treatment_df = self.treatment_df.reset_index()\n",
    "                self.treatment_df['timestamp'] = pd.to_datetime(self.treatment_df[\"created_at\"])\n",
    "            self.treatment_df['treatmentid'] = [i for i in range(len(self.treatment_df))]\n",
    "            return self.treatment_df\n",
    "        \n",
    "    def get_entries_shapes(self):\n",
    "        if self.entries_shapes is not None:\n",
    "            return self.entries_shapes\n",
    "        else:\n",
    "            self.entries_shapes = [pd.read_csv(file, low_memory=False).shape for file in self.entries_files]\n",
    "            return self.entries_shapes\n",
    "    \n",
    "    def get_entries_df(self):\n",
    "        if self.entries_df is not None:\n",
    "            return self.entries_df\n",
    "        else:\n",
    "            entries_dfs = [pd.read_csv(file, low_memory=False, header=None).reset_index(drop=True) for file in self.entries_files]\n",
    "            self.entries_df = pd.concat(entries_dfs, axis=0)\n",
    "            self.entries_df.columns = [\"time\", \"bg\"]\n",
    "            try:\n",
    "                self.entries_df['timestamp'] = pd.to_datetime(self.entries_df['time'])\n",
    "            except Exception:\n",
    "                print(\"HERE IN THE ERROR\")\n",
    "                self.entries_df['timestamp'] = pd.to_datetime(self.entries_df['time'].str.replace(\"PM\", \"\"))\n",
    "#                 self.entries_df['timestamp'] = pd.to_datetime(self.entries_df['time'])\n",
    "            self.entries_df['entryid']  = [i for i in range(len(self.entries_df))]\n",
    "            return self.entries_df\n",
    "        \n",
    "    def get_join_table(self):\n",
    "        if self.join_table is not None:\n",
    "            return self.join_table\n",
    "        else:\n",
    "            self.join_table = self._temporal_join()\n",
    "            return self.join_table\n",
    "\n",
    "    def _temporal_join(self):\n",
    "        if len(self.entries_files) == 0:\n",
    "            print(f\"No entries for subject {self.subject_id}. Returning None\")\n",
    "            return None\n",
    "        elif len(self.device_status_files) == 0 and len(self.treatment_files) == 0:\n",
    "            print(f\"{self.subject_id} does not have device status or treatment files. Returning None\")\n",
    "            return None\n",
    "        \n",
    "        # Load tables and convert relevant columns to date times\n",
    "        entry_df = self.get_entries_df()\n",
    "        treatments_df = self.get_treatment_df()\n",
    "        device_status_df = self.get_device_status_df()\n",
    "        \n",
    "        \n",
    "        if treatments_df.empty and device_status_df.empty:\n",
    "            print(f\"subject {self.subject_id} does not have treatment or device status tables. Passing.\")\n",
    "        # Creat\n",
    "        index_dict = self._temporal_join_index_dict(entries=entry_df, device_status=device_status_df, treatments=treatments_df)\n",
    "        join_df = self._create_temporal_join_df(index_dict)\n",
    "        \n",
    "        if device_status_df is not None and treatments_df is not None:\n",
    "            joined_data = (join_df\n",
    "                           .merge(entry_df, how='left', left_on ='entryid', right_on='entryid', suffixes=(\"_x\",\"_ent\"))\n",
    "                           .merge(device_status_df, how='left', left_on=\"devicestatusid\", right_on=\"devicestatusid\", suffixes=(\"_y\",\"_ds\")) \n",
    "                           .merge(treatments_df, how='left', left_on=\"treatmentid\", right_on=\"treatmentid\", suffixes=(\"_z\",\"_tre\")) \n",
    "                          )\n",
    "        elif device_status_df is None and treatments_df is not None:\n",
    "            joined_data = (join_df\n",
    "               .merge(entry_df, how='left', left_on ='entryid', right_on='entryid', suffixes=(\"_x\",\"_ent\"))\n",
    "               .merge(treatments_df, how='left', left_on=\"treatmentid\", right_on=\"treatmentid\", suffixes=(\"_z\",\"_tre\")) \n",
    "              )\n",
    "        elif device_status_df is not None and treatments_df is None:\n",
    "            joined_data = (join_df\n",
    "               .merge(entry_df, how='left', left_on ='entryid', right_on='entryid', suffixes=(\"_x\",\"_ent\"))\n",
    "               .merge(device_status_df, how='left', left_on=\"devicestatusid\", right_on=\"devicestatusid\", suffixes=(\"_y\",\"_ds\")) \n",
    "              )\n",
    "        return joined_data\n",
    "    \n",
    "    def _temporal_join_index_dict(self, entries, device_status, treatments):\n",
    "        \"\"\"Assign device status and treatment rows to the nearest entry that occurs after the device status or treatment row.\"\"\"\n",
    "        # Store timestamp and entries in zipped list; get entry timezones\n",
    "        timestamp_keys = entries['timestamp'].to_list()\n",
    "        timestamp_keys = [x.replace(tzinfo=pytz.utc) for x in timestamp_keys if isinstance(x, pd.Timestamp) or isinstance(x, datetime)]\n",
    "        entry_id_list = entries['entryid'].to_list()\n",
    "        zipped = list(zip(timestamp_keys, entry_id_list))\n",
    "        \n",
    "        # Check for offset aware \n",
    "        \n",
    "        # fill in standard python dictionary with entry data; convert to SortedDict sorted on entry timestamps\n",
    "        index_dict = SortedDict({timestamp: (entry_id, {\"device_status\": [], \"treatment\": []}) for timestamp, entry_id in zipped})\n",
    "        \n",
    "        # Generate list of tuples for (devicetimestamp, deviceid) \n",
    "        if device_status is not None:\n",
    "            device_tuples = list(zip(device_status['timestamp'], device_status['devicestatusid']))\n",
    "        else:\n",
    "            device_tuples = None\n",
    "        \n",
    "        # Generate list of tuples for (devicetimestamp, deviceid) \n",
    "        if treatments is not None:\n",
    "            treatments_tuples = list(zip(treatments['timestamp'], treatments['treatmentid']))\n",
    "        else:\n",
    "            treatments_tuples = None\n",
    "        \n",
    "        # Set constants from index_dict\n",
    "        index_keys = index_dict.keys()\n",
    "        max_idx = index_dict.index(index_keys[len(index_keys)-1])\n",
    "        \n",
    "        if device_tuples is not None:\n",
    "            for comparison_timestamp, comparison_id in device_tuples:\n",
    "                # Left idx is the index of the entry timestamp the comparison timestamp is less than or equal to \n",
    "                try:\n",
    "                    left_idx = index_dict.bisect_left(comparison_timestamp) \n",
    "                except TypeError:\n",
    "                    left_idx = index_dict.bisect_left(comparison_timestamp.replace(tzinfo=pytz.utc))\n",
    "\n",
    "                # Assign comparison timestamps greater than the last entry to the last entry\n",
    "                # (Comparisons < min(entry timestamp) will naturally be joined to min(entry timestamp))\n",
    "                if left_idx >= max_idx:\n",
    "                    left_idx = max_idx\n",
    "\n",
    "                # Get the index_dict key associated with the bisect_left operation\n",
    "                assignment_key = index_keys[left_idx]\n",
    "\n",
    "                # Assign the comparison_id to the assignment key of the index_dict\n",
    "                index_dict[assignment_key][1]['device_status'].append(comparison_id)\n",
    "        \n",
    "        if treatments_tuples is not None:\n",
    "            # Equivalent to the above for-loop but for treatments_tuples    \n",
    "            for comparison_timestamp, comparison_id in treatments_tuples:\n",
    "                # Left idx is the index of the entry timestamp the comparison timestamp is less than or equal to \n",
    "                try:\n",
    "                    left_idx = index_dict.bisect_left(comparison_timestamp) \n",
    "                except TypeError:\n",
    "                    left_idx = index_dict.bisect_left(comparison_timestamp.replace(tzinfo=pytz.utc))\n",
    "\n",
    "                # Assign comparison timestamps greater than the last entry to the last entry\n",
    "                # (Comparisons < min(entry timestamp) will naturally be joined to min(entry timestamp))\n",
    "                if left_idx >= max_idx:\n",
    "                    left_idx = max_idx\n",
    "\n",
    "                # Get the index_dict key associated with the bisect_left operation\n",
    "                assignment_key = index_keys[left_idx]\n",
    "\n",
    "                # Assign the comparison_id to the assignment key of the index_dict\n",
    "                index_dict[assignment_key][1]['treatment'].append(comparison_id)\n",
    "\n",
    "        return index_dict\n",
    "    \n",
    "    def _create_temporal_join_df(self, index_dict):\n",
    "        join_ids = []\n",
    "        keys = index_dict.keys()\n",
    "        for k in keys:\n",
    "            device_ids = index_dict[k][1]['device_status']\n",
    "            treatment_ids = index_dict[k][1]['treatment']\n",
    "\n",
    "            zip_lists = []\n",
    "            # if device_ids and treatment_ids have different lengths...\n",
    "            if len(device_ids) != len(treatment_ids):\n",
    "                # ... fill the shorter list with None values \n",
    "                ids = [device_ids, treatment_ids]\n",
    "                lengths = [len(i) for i in ids]\n",
    "                zip_length = max(lengths)\n",
    "\n",
    "                # Fill in the shortest list\n",
    "                min_len_idx = lengths.index(min(lengths))\n",
    "                min_len_ids = ids[min_len_idx]\n",
    "                min_len_ids.extend([None for _ in range(min_len_idx, zip_length)])\n",
    "\n",
    "                # identify the longest list \n",
    "                max_len_idx = lengths.index(max(lengths))\n",
    "                max_len_ids = ids[max_len_idx]\n",
    "\n",
    "                # Append all lists to zip_list ordered as (entry_ids, device_ids, treatments_ids) \n",
    "                zip_lists.append([index_dict[k][0] for _ in range(zip_length)]) # entry Id's\n",
    "\n",
    "                # Identify the device id list to facilitate appending lists in the correct order\n",
    "                if max_len_ids == device_ids:\n",
    "                    zip_lists.append(max_len_ids) # device_ids\n",
    "                    zip_lists.append(min_len_ids) # treatment_ids\n",
    "                else:\n",
    "                    zip_lists.append(min_len_ids) # device_ids\n",
    "                    zip_lists.append(max_len_ids) # treatment_ids\n",
    "            else:\n",
    "                # Set the zip_length to the length of device_ids\n",
    "                zip_length = len(device_ids)\n",
    "                zip_lists.append([index_dict[k][0] for _ in range(zip_length)]) # Entry_ids\n",
    "                zip_lists.append(device_ids)\n",
    "                zip_lists.append(treatment_ids)\n",
    "\n",
    "            # Add the current iteration results \n",
    "            join_ids.extend(list(zip(*zip_lists)))\n",
    "\n",
    "        join_dict = {\"entryid\": [i[0] for i in join_ids],\n",
    "                     \"devicestatusid\": [i[1] for i in join_ids],\n",
    "                     \"treatmentid\": [i[2] for i in join_ids]}\n",
    "        join_df = pd.DataFrame(join_dict)\n",
    "        return join_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "25cd7485-5433-4efd-b492-813e98b6032e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97872409\n"
     ]
    }
   ],
   "source": [
    "subject = [s for s in subject_dirs if s.subject_id==\"97872409\"][0]\n",
    "# subject = subject_dirs[1]\n",
    "sub = Subject(subject.subject_id, subject.path, subject.treatments, subject.devicestatus, subject.entries)\n",
    "print(sub.subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "931a5e42-071b-4c3f-9410-9845fe3c9d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    entries = sub.get_entries_df()\n",
    "    t = pd.to_datetime(entries[\"time\"])\n",
    "except OutOfBoundsDatetime:\n",
    "    print(\"here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b398c0f9-df7d-4eeb-8248-a5a284456321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out of bounds datetime\n",
      "(457970, 3)\n"
     ]
    }
   ],
   "source": [
    "entries = sub.get_entries_df()\n",
    "entries['time'] = entries['time'].str.replace(\"PM\", \"\")\n",
    "try:\n",
    "    t = pd.to_datetime(entries['time'])\n",
    "except OutOfBoundsDatetime:\n",
    "    print('out of bounds datetime')\n",
    "    entries['timestamp'] = pd.to_datetime(entries['time'], errors='coerce')\n",
    "    nulls = entries['timestamp'].loc[entries['timestamp'].isna(), ]\n",
    "    entries = entries.loc[~entries['timestamp'].isin(nulls)]\n",
    "    print(entries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4ee5ba66-989e-4844-b65f-2b6765e691be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>bg</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-21T19:31:00-04:00</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2017-07-21 19:31:00-04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-07-21T19:31:00-04:00</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2017-07-21 19:31:00-04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-07-21T19:31:00-04:00</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2017-07-21 19:31:00-04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-07-21T19:31:00-04:00</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2017-07-21 19:31:00-04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-07-21T19:31:00-04:00</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2017-07-21 19:31:00-04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275066</th>\n",
       "      <td>2017-04-03T19:00:30.000Z</td>\n",
       "      <td>105.0</td>\n",
       "      <td>2017-04-03 19:00:30+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275067</th>\n",
       "      <td>2017-04-03T18:57:30.000Z</td>\n",
       "      <td>105.0</td>\n",
       "      <td>2017-04-03 18:57:30+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275068</th>\n",
       "      <td>2017-04-03T18:42:30.000Z</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2017-04-03 18:42:30+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275069</th>\n",
       "      <td>2017-04-03T18:34:30.000Z</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2017-04-03 18:34:30+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275070</th>\n",
       "      <td>2017-04-03T18:31:30.000Z</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2017-04-03 18:31:30+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>457970 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             time     bg                  timestamp\n",
       "0       2017-07-21T19:31:00-04:00   96.0  2017-07-21 19:31:00-04:00\n",
       "1       2017-07-21T19:31:00-04:00   94.0  2017-07-21 19:31:00-04:00\n",
       "2       2017-07-21T19:31:00-04:00   96.0  2017-07-21 19:31:00-04:00\n",
       "3       2017-07-21T19:31:00-04:00   94.0  2017-07-21 19:31:00-04:00\n",
       "4       2017-07-21T19:31:00-04:00   96.0  2017-07-21 19:31:00-04:00\n",
       "...                           ...    ...                        ...\n",
       "275066   2017-04-03T19:00:30.000Z  105.0  2017-04-03 19:00:30+00:00\n",
       "275067   2017-04-03T18:57:30.000Z  105.0  2017-04-03 18:57:30+00:00\n",
       "275068   2017-04-03T18:42:30.000Z   94.0  2017-04-03 18:42:30+00:00\n",
       "275069   2017-04-03T18:34:30.000Z   80.0  2017-04-03 18:34:30+00:00\n",
       "275070   2017-04-03T18:31:30.000Z   95.0  2017-04-03 18:31:30+00:00\n",
       "\n",
       "[457970 rows x 3 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = entries['timestamp'].loc[entries['timestamp'].isna(),]\n",
    "\n",
    "entries.loc[~entries['timestamp'].isin(test), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f6692ca3-43ef-44dd-91e6-e7b05e5cd5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(457991,)\n",
      "(457991,)\n",
      "True\n",
      "(457970,)\n",
      "Int64Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print(t.shape)\n",
    "print(t.isna().shape)\n",
    "print((t.index == t.isna().index).all() )\n",
    "test =t.dropna()\n",
    "print(test.shape)\n",
    "drop_indices = test.index. difference(entries.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "36269a73-8f53-4385-ba64-1d36d339bfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE IN THE ERROR\n"
     ]
    },
    {
     "ename": "OutOfBoundsDatetime",
     "evalue": "Out of bounds nanosecond timestamp: 117-07-31 05:01:13",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[1;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object, allow_mixed)\u001b[0m\n\u001b[0;32m   2191\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2192\u001b[1;33m             \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz_parsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime_to_datetime64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"K\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2193\u001b[0m             \u001b[1;31m# If tzaware, these values represent unix timestamps, so we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslibs\\conversion.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.conversion.datetime_to_datetime64\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unrecognized value type: <class 'str'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutOfBoundsDatetime\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30576/4075449201.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_join_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30576/2078689850.py\u001b[0m in \u001b[0;36mget_join_table\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_temporal_join\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30576/2078689850.py\u001b[0m in \u001b[0;36m_temporal_join\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;31m# Load tables and convert relevant columns to date times\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[0mentry_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_entries_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[0mtreatments_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_treatment_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mdevice_status_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_device_status_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30576/2078689850.py\u001b[0m in \u001b[0;36mget_entries_df\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"HERE IN THE ERROR\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentries_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentries_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PM\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;31m#                 self.entries_df['timestamp'] = pd.to_datetime(self.entries_df['time'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentries_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entryid'\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentries_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m    881\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz_localize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtz\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 883\u001b[1;33m         \u001b[0mcache_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_maybe_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcache_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py\u001b[0m in \u001b[0;36m_maybe_cache\u001b[1;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0munique_dates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m             \u001b[0mcache_dates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m             \u001b[0mcache_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_dates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[1;31m# GH#39882 and GH#35888 in case of None and NaT we get duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minfer_datetime_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[0mutc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtz\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"utc\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m     result, tz_parsed = objects_to_datetime64ns(\n\u001b[0m\u001b[0;32m    402\u001b[0m         \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[0mdayfirst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdayfirst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[1;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object, allow_mixed)\u001b[0m\n\u001b[0;32m   2196\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"i8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz_parsed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2197\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2198\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2200\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtz_parsed\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[1;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object, allow_mixed)\u001b[0m\n\u001b[0;32m   2178\u001b[0m     \u001b[0morder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"F\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"C\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"F\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"C\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2180\u001b[1;33m         result, tz_parsed = tslib.array_to_datetime(\n\u001b[0m\u001b[0;32m   2181\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"K\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2182\u001b[0m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslibs\\np_datetime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.np_datetime.check_dts_bounds\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOutOfBoundsDatetime\u001b[0m: Out of bounds nanosecond timestamp: 117-07-31 05:01:13"
     ]
    }
   ],
   "source": [
    "test = sub.get_join_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3ea99479-0395-4c5f-ab35-87efa5dbb1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entryid</th>\n",
       "      <th>devicestatusid</th>\n",
       "      <th>treatmentid</th>\n",
       "      <th>time</th>\n",
       "      <th>bg_z</th>\n",
       "      <th>timestamp_z</th>\n",
       "      <th>created_at</th>\n",
       "      <th>absolute</th>\n",
       "      <th>carbs</th>\n",
       "      <th>_id</th>\n",
       "      <th>...</th>\n",
       "      <th>square/appended/0/_head</th>\n",
       "      <th>square/appended/0/_body</th>\n",
       "      <th>square/appended/0/_date</th>\n",
       "      <th>square/duration</th>\n",
       "      <th>square/_description</th>\n",
       "      <th>square/programmed</th>\n",
       "      <th>square/amount</th>\n",
       "      <th>square/_head</th>\n",
       "      <th>square/_body</th>\n",
       "      <th>square/_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>198303</td>\n",
       "      <td>None</td>\n",
       "      <td>80582</td>\n",
       "      <td>Mon Jan 29 08:20:59 CET 2018</td>\n",
       "      <td>146</td>\n",
       "      <td>2018-01-29 08:20:59</td>\n",
       "      <td>2018-01-29T08:45:28+01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5b099a36b7c2fc0994a1ce4f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198303</td>\n",
       "      <td>None</td>\n",
       "      <td>80583</td>\n",
       "      <td>Mon Jan 29 08:20:59 CET 2018</td>\n",
       "      <td>146</td>\n",
       "      <td>2018-01-29 08:20:59</td>\n",
       "      <td>2018-01-29T08:45:25+01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5b00b3af586b983f516ffd5d</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>198303</td>\n",
       "      <td>None</td>\n",
       "      <td>80584</td>\n",
       "      <td>Mon Jan 29 08:20:59 CET 2018</td>\n",
       "      <td>146</td>\n",
       "      <td>2018-01-29 08:20:59</td>\n",
       "      <td>2018-01-29T07:33:13+01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42e4f7a264494bca8d8669af</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>198303</td>\n",
       "      <td>None</td>\n",
       "      <td>80585</td>\n",
       "      <td>Mon Jan 29 08:20:59 CET 2018</td>\n",
       "      <td>146</td>\n",
       "      <td>2018-01-29 08:20:59</td>\n",
       "      <td>2018-01-29T07:33:13+01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2bc7f6cbf1f5431281b9423d</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198303</td>\n",
       "      <td>None</td>\n",
       "      <td>80586</td>\n",
       "      <td>Mon Jan 29 08:20:59 CET 2018</td>\n",
       "      <td>146</td>\n",
       "      <td>2018-01-29 08:20:59</td>\n",
       "      <td>2018-01-29T07:33:13+01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5b00b3af586b983f516ffd4e</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80906</th>\n",
       "      <td>62</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-05-10T23:20:19.259+0200</td>\n",
       "      <td>276</td>\n",
       "      <td>2019-05-10 23:20:19.259000+02:00</td>\n",
       "      <td>2019-05-10T23:17:56Z</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5cd69d5a131daf5594f593cc</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80907</th>\n",
       "      <td>58</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>2019-05-10T23:30:19.259+0200</td>\n",
       "      <td>274</td>\n",
       "      <td>2019-05-10 23:30:19.259000+02:00</td>\n",
       "      <td>2019-05-10T23:29:01Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5cd69da3131daf5594f593e4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80908</th>\n",
       "      <td>56</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-05-10T23:35:19.259+0200</td>\n",
       "      <td>272</td>\n",
       "      <td>2019-05-10 23:35:19.259000+02:00</td>\n",
       "      <td>2019-05-10T23:30:53Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5cd69da3131daf5594f593e2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80909</th>\n",
       "      <td>54</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-10T23:40:19.259+0200</td>\n",
       "      <td>270</td>\n",
       "      <td>2019-05-10 23:40:19.259000+02:00</td>\n",
       "      <td>2019-05-10T23:39:03Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5cd69da3131daf5594f593e3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80910</th>\n",
       "      <td>52</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-05-10T23:45:19.259+0200</td>\n",
       "      <td>264</td>\n",
       "      <td>2019-05-10 23:45:19.259000+02:00</td>\n",
       "      <td>2019-05-10T23:40:47Z</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5cd69da3131daf5594f593e1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80911 rows × 359 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       entryid devicestatusid  treatmentid                          time  \\\n",
       "0       198303           None        80582  Mon Jan 29 08:20:59 CET 2018   \n",
       "1       198303           None        80583  Mon Jan 29 08:20:59 CET 2018   \n",
       "2       198303           None        80584  Mon Jan 29 08:20:59 CET 2018   \n",
       "3       198303           None        80585  Mon Jan 29 08:20:59 CET 2018   \n",
       "4       198303           None        80586  Mon Jan 29 08:20:59 CET 2018   \n",
       "...        ...            ...          ...                           ...   \n",
       "80906       62           None            4  2019-05-10T23:20:19.259+0200   \n",
       "80907       58           None            3  2019-05-10T23:30:19.259+0200   \n",
       "80908       56           None            2  2019-05-10T23:35:19.259+0200   \n",
       "80909       54           None            1  2019-05-10T23:40:19.259+0200   \n",
       "80910       52           None            0  2019-05-10T23:45:19.259+0200   \n",
       "\n",
       "       bg_z                       timestamp_z                 created_at  \\\n",
       "0       146               2018-01-29 08:20:59  2018-01-29T08:45:28+01:00   \n",
       "1       146               2018-01-29 08:20:59  2018-01-29T08:45:25+01:00   \n",
       "2       146               2018-01-29 08:20:59  2018-01-29T07:33:13+01:00   \n",
       "3       146               2018-01-29 08:20:59  2018-01-29T07:33:13+01:00   \n",
       "4       146               2018-01-29 08:20:59  2018-01-29T07:33:13+01:00   \n",
       "...     ...                               ...                        ...   \n",
       "80906   276  2019-05-10 23:20:19.259000+02:00       2019-05-10T23:17:56Z   \n",
       "80907   274  2019-05-10 23:30:19.259000+02:00       2019-05-10T23:29:01Z   \n",
       "80908   272  2019-05-10 23:35:19.259000+02:00       2019-05-10T23:30:53Z   \n",
       "80909   270  2019-05-10 23:40:19.259000+02:00       2019-05-10T23:39:03Z   \n",
       "80910   264  2019-05-10 23:45:19.259000+02:00       2019-05-10T23:40:47Z   \n",
       "\n",
       "       absolute  carbs                       _id  ...  \\\n",
       "0           NaN    NaN  5b099a36b7c2fc0994a1ce4f  ...   \n",
       "1           NaN    NaN  5b00b3af586b983f516ffd5d  ...   \n",
       "2           NaN    NaN  42e4f7a264494bca8d8669af  ...   \n",
       "3           NaN    NaN  2bc7f6cbf1f5431281b9423d  ...   \n",
       "4           NaN    NaN  5b00b3af586b983f516ffd4e  ...   \n",
       "...         ...    ...                       ...  ...   \n",
       "80906       0.0    NaN  5cd69d5a131daf5594f593cc  ...   \n",
       "80907       NaN    NaN  5cd69da3131daf5594f593e4  ...   \n",
       "80908       NaN    NaN  5cd69da3131daf5594f593e2  ...   \n",
       "80909       NaN    NaN  5cd69da3131daf5594f593e3  ...   \n",
       "80910       0.0    NaN  5cd69da3131daf5594f593e1  ...   \n",
       "\n",
       "       square/appended/0/_head  square/appended/0/_body  \\\n",
       "0                          NaN                      NaN   \n",
       "1                          NaN                      NaN   \n",
       "2                          NaN                      NaN   \n",
       "3                          NaN                      NaN   \n",
       "4                          NaN                      NaN   \n",
       "...                        ...                      ...   \n",
       "80906                      NaN                      NaN   \n",
       "80907                      NaN                      NaN   \n",
       "80908                      NaN                      NaN   \n",
       "80909                      NaN                      NaN   \n",
       "80910                      NaN                      NaN   \n",
       "\n",
       "      square/appended/0/_date square/duration square/_description  \\\n",
       "0                         NaN             NaN                 NaN   \n",
       "1                         NaN             NaN                 NaN   \n",
       "2                         NaN             NaN                 NaN   \n",
       "3                         NaN             NaN                 NaN   \n",
       "4                         NaN             NaN                 NaN   \n",
       "...                       ...             ...                 ...   \n",
       "80906                     NaN             NaN                 NaN   \n",
       "80907                     NaN             NaN                 NaN   \n",
       "80908                     NaN             NaN                 NaN   \n",
       "80909                     NaN             NaN                 NaN   \n",
       "80910                     NaN             NaN                 NaN   \n",
       "\n",
       "       square/programmed  square/amount square/_head  square/_body  \\\n",
       "0                    NaN            NaN          NaN           NaN   \n",
       "1                    NaN            NaN          NaN           NaN   \n",
       "2                    NaN            NaN          NaN           NaN   \n",
       "3                    NaN            NaN          NaN           NaN   \n",
       "4                    NaN            NaN          NaN           NaN   \n",
       "...                  ...            ...          ...           ...   \n",
       "80906                NaN            NaN          NaN           NaN   \n",
       "80907                NaN            NaN          NaN           NaN   \n",
       "80908                NaN            NaN          NaN           NaN   \n",
       "80909                NaN            NaN          NaN           NaN   \n",
       "80910                NaN            NaN          NaN           NaN   \n",
       "\n",
       "      square/_date  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  \n",
       "...            ...  \n",
       "80906          NaN  \n",
       "80907          NaN  \n",
       "80908          NaN  \n",
       "80909          NaN  \n",
       "80910          NaN  \n",
       "\n",
       "[80911 rows x 359 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b1b85b2a-ccac-49d5-824d-1d42670cce1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('b',)\n"
     ]
    }
   ],
   "source": [
    "test = {'a': 1}\n",
    "try:\n",
    "    test['b']\n",
    "except KeyError as e:\n",
    "    print(e.args)\n",
    "#     if \"KeyError\" in e.args:\n",
    "#         print('asdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "588b2e0f-f4cc-4103-91c0-c11b4bdf5ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_run_start = datetime.now()\n",
    "# i =0 \n",
    "# for x in subject_dirs[14:]:\n",
    "#     print(\"iteration\", i)\n",
    "#     iter_start = datetime.now()\n",
    "#     sub_id = x.subject_id\n",
    "#     subject = Subject(x.subject_id, x.path, x.treatments, x.devicestatus, x.entries)\n",
    "#     joined_data = subject.get_join_table()\n",
    "#     print(joined_data.shape)\n",
    "#     run_time = datetime.now()-iter_start\n",
    "#     print(f\"Run time for subject {sub_id} = {run_time}\")\n",
    "#     i+=1\n",
    "# print(f\"Full run time: {datetime.now() - full_run_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a54adbc1-8ce8-4f84-97eb-d42fb3e93203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 455), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 457), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 454), (15000, 455), (15000, 454), (15000, 556), (15000, 445), (15000, 446), (14547, 312), (15000, 443), (15000, 443), (15000, 441), (15000, 371), (15000, 300), (15000, 124), (15000, 42), (13461, 3), (15000, 124), (15000, 123), (15000, 121), (15000, 120), (15000, 121), (15000, 121), (11615, 154)]\n",
      "[(15000, 438), (15000, 389), (15000, 501), (15000, 424), (15000, 364), (15000, 384), (4446, 365), (15000, 29), (15000, 32), (15000, 29), (15000, 31), (15000, 32), (15000, 460), (8497, 361)]\n"
     ]
    }
   ],
   "source": [
    "subject = Subject(x.subject_id, x.path, x.treatments, x.devicestatus, x.entries)\n",
    "print(subject.get_device_status_shapes())\n",
    "print(subject.get_treatment_shapes())\n",
    "# entries = subject.get_entries_df()\n",
    "# timestamp_keys = entries['timestamp'].to_list()\n",
    "# [x.replace(tzinfo=pytz.utc) for x in timestamp_keys if type(x)==datetime]\n",
    "# [type(x) for x in timestamp_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "64d9746a-4ba1-4e8b-8c4e-504f990d99ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(timestamp_keys[0]) == datetime\n",
    "isinstance(timestamp_keys[0], pd.Timestamp) or isinstance(timestamp_keys[0], datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "64a036d1-e4f4-403c-9487-070dc3ef092d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233642"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry_df = subject.get_entries_df()\n",
    "t = entry_df.timestamp.to_list()\n",
    "l = [x for x in t if type(x)==datetime]\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a1d7fc-e562-4383-a981-f6e33a376642",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_keys = subject.get_entries_df()['timestamp'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae3dda-6492-41ca-a099-7be10b25ca75",
   "metadata": {},
   "source": [
    "**Below this point, the cells will not run. These cells were used to test different elements of Subject.get_temporal_join()**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e745ee05-4d94-4adb-b69c-84a357064bd9",
   "metadata": {},
   "source": [
    "In the below cell, it's curious that we see the same treatment and device status ID's given that these are independently created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f00b99c3-6aa5-4471-a0b5-0e118a1ee264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113104, {'device_status': [199581, 199582, 199583, 199584], 'treatment': [8889]})\n",
      "(108504, {'device_status': [190469, 190470, 190471, 190472], 'treatment': [8886, 8887, 8888]})\n",
      "(108500, {'device_status': [190461, 190462, 190463, 190464], 'treatment': [8884, 8885]})\n",
      "(108472, {'device_status': [190405, 190406, 190407, 190408], 'treatment': [8883]})\n",
      "(108462, {'device_status': [190385, 190386, 190387, 190388], 'treatment': [8881, 8882]})\n",
      "(108454, {'device_status': [190369, 190370, 190371, 190372], 'treatment': [8879, 8880]})\n",
      "(108431, {'device_status': [190324, 190325, 190326, 190327], 'treatment': [8878]})\n",
      "(108427, {'device_status': [190314, 190315, 190316, 190317, 190318, 190319], 'treatment': [8877]})\n",
      "(108407, {'device_status': [190274, 190275, 190276, 190277], 'treatment': [8875, 8876]})\n",
      "(108403, {'device_status': [190266, 190267, 190268, 190269], 'treatment': [8870, 8871, 8872, 8873, 8874]})\n",
      "(108389, {'device_status': [190242, 190243, 190244, 190245], 'treatment': [8867, 8868, 8869]})\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for k,v in test.items():\n",
    "    if len(v[1][\"device_status\"]) and len(v[1]['treatment']):\n",
    "        print(v)\n",
    "        i +=1\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d51e73-db63-4dff-935b-934667a4ca2b",
   "metadata": {},
   "source": [
    "Based on the cell below, it appears the device status and treatment ID's are never different where there are matched values. It appears that device status and treatment data both align with a portion of the entries data, AND they align with the same portion of the entries data. Therefore, we have some subset of entries data with treatment and device status data most of the time. (I re-ran this process for several subjects). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1516e7d0-1402-40b7-a5cb-74d522241bc0",
   "metadata": {},
   "source": [
    "Now, let's look into making the join dataframe from the temporal_join output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "590438eb-c7d0-4a75-9127-3f607b0f32d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218195\n",
      "218195\n",
      "8889\n",
      "8889\n"
     ]
    }
   ],
   "source": [
    "# Validate unique ID's \n",
    "keys = test.keys()\n",
    "unique_ds = set()\n",
    "not_unique_ds = []\n",
    "unique_treat =set()\n",
    "not_unique_treat = []\n",
    "for k,v in test.items():\n",
    "    ds = v[1]['device_status']\n",
    "    if ds:\n",
    "        [unique_ds.add(i) for i in ds if i]\n",
    "        [not_unique_ds.append(i) for i in ds if i]\n",
    "    treat = v[1]['treatment']\n",
    "    if treat:\n",
    "#         print(treat)\n",
    "        [unique_treat.add(i) for i in treat if i]\n",
    "        [not_unique_treat.append(i) for i in treat if i]\n",
    "print(len(unique_ds))\n",
    "print(len(not_unique_ds))\n",
    "print(len(unique_treat))\n",
    "print(len(not_unique_treat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ecf3dda-2957-4dc6-9493-6cf7a4780c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entryid</th>\n",
       "      <th>devicestatusid</th>\n",
       "      <th>treatmentid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121768</td>\n",
       "      <td>218190.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121768</td>\n",
       "      <td>218191.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>121768</td>\n",
       "      <td>218192.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121764</td>\n",
       "      <td>218186.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121764</td>\n",
       "      <td>218187.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163236</th>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163237</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163238</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163239</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163240</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163241 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        entryid  devicestatusid  treatmentid\n",
       "0        121768        218190.0          NaN\n",
       "1        121768        218191.0          NaN\n",
       "2        121768        218192.0          NaN\n",
       "3        121764        218186.0          NaN\n",
       "4        121764        218187.0          NaN\n",
       "...         ...             ...          ...\n",
       "163236        1             9.0          NaN\n",
       "163237        0             0.0          NaN\n",
       "163238        0             1.0          NaN\n",
       "163239        0             2.0          NaN\n",
       "163240        0             3.0          NaN\n",
       "\n",
       "[163241 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify to not assume equal length treatments/device_status \n",
    "join_ids = []\n",
    "for k in keys:\n",
    "    device_ids = test[k][1]['device_status']\n",
    "    treatment_ids = test[k][1]['treatment']\n",
    "    \n",
    "    zip_lists = []\n",
    "    # if device_ids and treatment_ids have different lengths...\n",
    "    if len(device_ids) != len(treatment_ids):\n",
    "        # ... fill the shorter list with None values \n",
    "        ids = [device_ids, treatment_ids]\n",
    "        lengths = [len(i) for i in ids]\n",
    "        zip_length = max(lengths)\n",
    "        \n",
    "        # Fill in the shortest list\n",
    "        min_len_idx = lengths.index(min(lengths))\n",
    "        min_len_ids = ids[min_len_idx]\n",
    "        min_len_ids.extend([None for _ in range(min_len_idx, zip_length)])\n",
    "        \n",
    "        # identify the longest list \n",
    "        max_len_idx = lengths.index(max(lengths))\n",
    "        max_len_ids = ids[max_len_idx]\n",
    "        \n",
    "        # Append all lists to zip_list ordered as (entry_ids, device_ids, treatments_ids) \n",
    "        zip_lists.append([test[k][0] for _ in range(zip_length)]) # entry Id's\n",
    "        \n",
    "        # Identify the device id list to facilitate appending lists in the correct order\n",
    "        if max_len_ids == device_ids:\n",
    "            zip_lists.append(max_len_ids) # device_ids\n",
    "            zip_lists.append(min_len_ids) # treatment_ids\n",
    "        else:\n",
    "            zip_lists.append(min_len_ids) # device_ids\n",
    "            zip_lists.append(max_len_ids) # treatment_ids\n",
    "    else:\n",
    "        # Set the zip_length to the length of device_ids\n",
    "        zip_length = len(device_ids)\n",
    "        zip_lists.append([test[k][0] for _ in range(zip_length)]) # Entry_ids\n",
    "        zip_lists.append(device_ids)\n",
    "        zip_lists.append(treatment_ids)\n",
    "\n",
    "    # Add the current iteration results \n",
    "    join_ids.extend(list(zip(*zip_lists)))\n",
    "    \n",
    "join_dict = {\"entryid\": [i[0] for i in join_ids],\n",
    "             \"devicestatusid\": [i[1] for i in join_ids],\n",
    "             \"treatmentid\": [i[2] for i in join_ids]}\n",
    "join_df = pd.DataFrame(join_dict)\n",
    "join_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36fcef1f-b9f2-4239-8c2c-95de67342fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163241\n",
      "163241\n"
     ]
    }
   ],
   "source": [
    "print(len(join_df))\n",
    "print(len(join_df.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6cdcbd7-e734-4b76-b282-5c6f650ba6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df = sub.get_device_status_df()\n",
    "treat_df = sub.get_treatment_df()\n",
    "entry_df = sub.get_entries_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a0ad106-e489-420f-aa1b-d7dca7202c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entryid</th>\n",
       "      <th>devicestatusid</th>\n",
       "      <th>treatmentid</th>\n",
       "      <th>time</th>\n",
       "      <th>bg</th>\n",
       "      <th>timestamp_y</th>\n",
       "      <th>pump/status/status</th>\n",
       "      <th>pump/status/timestamp</th>\n",
       "      <th>pump/clock</th>\n",
       "      <th>pump/reservoir</th>\n",
       "      <th>...</th>\n",
       "      <th>splitNow</th>\n",
       "      <th>enteredinsulin</th>\n",
       "      <th>relative</th>\n",
       "      <th>uuid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sysTime</th>\n",
       "      <th>isAnnouncement</th>\n",
       "      <th>CircadianPercentageProfile</th>\n",
       "      <th>percentage</th>\n",
       "      <th>timeshift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121768</td>\n",
       "      <td>218190.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-02-01T23:21:19.140+1000</td>\n",
       "      <td>180</td>\n",
       "      <td>2019-02-01 23:21:19.140000+10:00</td>\n",
       "      <td>normal</td>\n",
       "      <td>2019-02-01T13:16:32Z</td>\n",
       "      <td>2019-02-01T13:16:32Z</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121768</td>\n",
       "      <td>218191.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-02-01T23:21:19.140+1000</td>\n",
       "      <td>180</td>\n",
       "      <td>2019-02-01 23:21:19.140000+10:00</td>\n",
       "      <td>normal</td>\n",
       "      <td>2019-02-01T13:16:31Z</td>\n",
       "      <td>2019-02-01T13:16:31Z</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>121768</td>\n",
       "      <td>218192.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-02-01T23:21:19.140+1000</td>\n",
       "      <td>180</td>\n",
       "      <td>2019-02-01 23:21:19.140000+10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121764</td>\n",
       "      <td>218186.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-02-01T13:36:12.000Z</td>\n",
       "      <td>199</td>\n",
       "      <td>2019-02-01 13:36:12+00:00</td>\n",
       "      <td>normal</td>\n",
       "      <td>2019-02-01T13:31:28Z</td>\n",
       "      <td>2019-02-01T13:31:28Z</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121764</td>\n",
       "      <td>218187.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-02-01T13:36:12.000Z</td>\n",
       "      <td>199</td>\n",
       "      <td>2019-02-01 13:36:12+00:00</td>\n",
       "      <td>normal</td>\n",
       "      <td>2019-02-01T13:31:28Z</td>\n",
       "      <td>2019-02-01T13:31:28Z</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 765 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   entryid  devicestatusid  treatmentid                          time    bg  \\\n",
       "0   121768        218190.0          NaN  2019-02-01T23:21:19.140+1000   180   \n",
       "1   121768        218191.0          NaN  2019-02-01T23:21:19.140+1000   180   \n",
       "2   121768        218192.0          NaN  2019-02-01T23:21:19.140+1000   180   \n",
       "3   121764        218186.0          NaN      2019-02-01T13:36:12.000Z   199   \n",
       "4   121764        218187.0          NaN      2019-02-01T13:36:12.000Z   199   \n",
       "\n",
       "                        timestamp_y pump/status/status pump/status/timestamp  \\\n",
       "0  2019-02-01 23:21:19.140000+10:00             normal  2019-02-01T13:16:32Z   \n",
       "1  2019-02-01 23:21:19.140000+10:00             normal  2019-02-01T13:16:31Z   \n",
       "2  2019-02-01 23:21:19.140000+10:00                NaN                   NaN   \n",
       "3         2019-02-01 13:36:12+00:00             normal  2019-02-01T13:31:28Z   \n",
       "4         2019-02-01 13:36:12+00:00             normal  2019-02-01T13:31:28Z   \n",
       "\n",
       "             pump/clock  pump/reservoir  ...  splitNow enteredinsulin  \\\n",
       "0  2019-02-01T13:16:32Z            50.0  ...       NaN            NaN   \n",
       "1  2019-02-01T13:16:31Z            50.0  ...       NaN            NaN   \n",
       "2                   NaN             NaN  ...       NaN            NaN   \n",
       "3  2019-02-01T13:31:28Z            50.0  ...       NaN            NaN   \n",
       "4  2019-02-01T13:31:28Z            50.0  ...       NaN            NaN   \n",
       "\n",
       "  relative uuid timestamp  sysTime isAnnouncement  CircadianPercentageProfile  \\\n",
       "0      NaN  NaN       NaT      NaN            NaN                         NaN   \n",
       "1      NaN  NaN       NaT      NaN            NaN                         NaN   \n",
       "2      NaN  NaN       NaT      NaN            NaN                         NaN   \n",
       "3      NaN  NaN       NaT      NaN            NaN                         NaN   \n",
       "4      NaN  NaN       NaT      NaN            NaN                         NaN   \n",
       "\n",
       "   percentage  timeshift  \n",
       "0         NaN        NaN  \n",
       "1         NaN        NaN  \n",
       "2         NaN        NaN  \n",
       "3         NaN        NaN  \n",
       "4         NaN        NaN  \n",
       "\n",
       "[5 rows x 765 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# latest per device status\n",
    "joined = (join_df\n",
    "          .merge(entry_df, how='left', left_on ='entryid', right_on='entryid', suffixes=(\"_x\",\"_ent\")) # drop duplicates\n",
    "          .merge(ds_df, how='left', left_on=\"devicestatusid\", right_on=\"devicestatusid\", suffixes=(\"_y\",\"_ds\")) # drop duplicates (sum columns left then groupby)\n",
    "          .merge(treat_df, how='left', left_on=\"treatmentid\", right_on=\"treatmentid\", suffixes=(\"_z\",\"_tre\")) # drop duplicates \n",
    "         )\n",
    "joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "012177b8-344f-4159-a06b-47fb0c40d78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163241, 765)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8e83d4e-f084-450d-922d-ae62b0fed985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122782, 4)\n",
      "(218196, 696)\n",
      "(8890, 65)\n",
      "59365\n",
      "163241\n"
     ]
    }
   ],
   "source": [
    "print(entry_df.shape)\n",
    "print(ds_df.shape)\n",
    "print(treat_df.shape)\n",
    "print(len(joined.entryid.drop_duplicates()))\n",
    "print(len(joined.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "894c75b7-202d-4351-85ed-86f9ce24b512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122782\n",
      "59365\n",
      "120415\n"
     ]
    }
   ],
   "source": [
    "entry_count = len(np.unique(entry_df.entryid))\n",
    "join_count = len(np.unique(joined.entryid))\n",
    "print(entry_count)\n",
    "print(join_count)\n",
    "unique_entry_ids = set()\n",
    "for k,v in test.items():\n",
    "    unique_entry_ids.add(v[0])\n",
    "print(len(unique_entry_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
