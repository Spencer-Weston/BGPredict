{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ca1ade-27df-437b-b5f2-ce8f7dfb1848",
   "metadata": {},
   "source": [
    "# Aggregate Night Scout Statistics\n",
    "Author: Spencer Weston\n",
    "\n",
    "This notebook exists to begin evaluating the nightscout/openaps data as a whole. This notebook should help determine which variables we can drop and generate some metadata. We also need to validate some assumptions across all subjects. We'll get setup by accumulating all the file pathes into a single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc46f502-95d6-4d58-aa48-9d27f71b1be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re \n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pytz\n",
    "from collections import namedtuple\n",
    "from sortedcontainers import SortedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec289c8-7ec3-41f3-8eb5-9cd50a3ac973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\spenc\\\\Documents\\\\Berkeley\\\\Capstone\\\\BGPredict\\\\Notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e287e5-2502-4ed4-9c47-2bc98fb2478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SubjectInfo = namedtuple(\"SubjectInfo\", ['path', 'subject_id', 'devicestatus', 'entries', 'treatments'])\n",
    "\n",
    "def generate_subject_info(data_dir, subject_number):\n",
    "    direct_sharing_folder = 'direct-sharing-31'\n",
    "    path = f\"{data_dir}/{subject_number}\"\n",
    "    # Some individuals have multiple direct-sharing folders. Ensure the correct folder exists and add it to path\n",
    "    # For example, one individual has a folder with menstruation data\n",
    "    if direct_sharing_folder not in os.listdir(path):\n",
    "        return None\n",
    "    else:\n",
    "        path = f\"{path}/{direct_sharing_folder}\"\n",
    "    \n",
    "    subject_dirs = [folder for folder in os.listdir(path)]\n",
    "    relevant_dir_names = [\"treatments\", \"devicestatus\", \"entries\"]\n",
    "    relevant_dirs = {k: [] for k in relevant_dir_names}\n",
    "    # Identify every file associated with treatments, device status, or entries and store them in a dictionary\n",
    "    for folder in subject_dirs:\n",
    "        for dir_name in relevant_dir_names:\n",
    "            if dir_name in folder and folder.endswith(\"_csv\"):\n",
    "                dir_path = f\"{path}/{folder}\"\n",
    "                files = [f\"{dir_path}/{file}\" for file in os.listdir(dir_path)]\n",
    "                relevant_dirs[dir_name].extend(files)\n",
    "    return SubjectInfo(path=path, subject_id=subject_number, devicestatus=relevant_dirs['devicestatus'],\n",
    "                      entries=relevant_dirs['entries'], treatments=relevant_dirs['treatments'])\n",
    "    \n",
    "# subject = SubjectInfo()\n",
    "data_dir = \"C:\\\\Users\\spenc\\Documents\\Berkeley\\Capstone\\n=183_OpenAPS_Data_Commons_August_2021_UNZIPPED\"\n",
    "data_dir = data_dir.replace(\"\\\\\", \"/\")\n",
    "data_dir = data_dir.replace(\"\\n\", \"/n\")\n",
    "# isnumeric validates that the folder comes from openaps/nightscout\n",
    "subject_dirs = [generate_subject_info(data_dir, folder) for folder in os.listdir(data_dir) if folder.isnumeric()] \n",
    "\n",
    "# None will be returned if an individual has no  \n",
    "try:\n",
    "    while subject_dirs.index(None):\n",
    "        idx = subject_dirs.index(None)\n",
    "        subject_dirs.pop(idx)\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7249130-d7fe-4c85-8091-ae9233be1ddc",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "Create some rough summary statistics based on the available files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b442747-4f88-48a2-9f0c-6814c78805f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_presence = {\"treatments\": [], \"devicestatus\": [], \"entries\": []}\n",
    "for x in subject_dirs:\n",
    "    for key in file_presence.keys():\n",
    "        tuple_idx = x._fields.index(key)\n",
    "        files = x[tuple_idx]\n",
    "        file_presence[key].append(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb68e40-0fb7-414c-9abf-63619cbeaca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['treatments', 'devicestatus', 'entries'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_presence.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024be9fe-8635-4127-a5f5-cd953c4c405f",
   "metadata": {},
   "source": [
    "Here, we look at subjects with no device status but treatment data and subjects with no folders associated with their data. If they have no folders, that's an easy exclusion criteria. If they have treatment but not device status data, we'll need to validate rather the treatment data appears equivalently formatted to the device status from other olders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12cf784b-e463-4b38-a229-66ef1832e5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDT: ['42052178', '50311906', '61179686', '66773091'] \n",
      " \n",
      " NF: ['32635618', '51359431'] \n",
      " \n",
      " NE: []\n"
     ]
    }
   ],
   "source": [
    "# Get count of the number of files for each subject\n",
    "subjects = [x.subject_id for x in subject_dirs]\n",
    "file_count = list(zip(subjects, *[file_presence[k] for k in file_presence.keys()]))\n",
    "\n",
    "# subjects with no device status but treatment data; will need to be evaluated for data integrity\n",
    "no_device_but_treatment = [] \n",
    "# Subjects with no folders associated with their data; exclusion criterion\n",
    "no_folders = [] \n",
    "# No blood glucose data; exclusion criterion \n",
    "no_entries = []\n",
    "\n",
    "for x in file_count:\n",
    "    # no entries\n",
    "    if x[1] < 1:\n",
    "        if x[2] == 0 and x[3] == 0:\n",
    "            no_folders.append(x[0])\n",
    "        else: \n",
    "            no_entries.append(x[0])\n",
    "        next \n",
    "    # no device status but treatment \n",
    "    if x[2] == 0 and x[3] > 0:\n",
    "        no_device_but_treatment.append(x[0])\n",
    "        next\n",
    "\n",
    "print(f\"NDT: {no_device_but_treatment} \\n \\n NF: {no_folders} \\n \\n NE: {no_entries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df8ede-7897-411e-b764-e1a8d8ad0b32",
   "metadata": {},
   "source": [
    "### Evaluate Subjects with No Device Status but Treatment Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "637412c0-947e-457d-852d-698242556913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from NightScoutJoinAnalysis\n",
    "def define_column_superset(dataframes: list):\n",
    "    superset = set()\n",
    "    for df in dataframes:\n",
    "        cols = list(df.columns)\n",
    "        superset = superset.union(cols)\n",
    "    return superset\n",
    "\n",
    "def apply_superset(df, superset):\n",
    "    df_cols = set(list(df.columns))\n",
    "    set_diff = superset.difference(df_cols)\n",
    "    n = len(df)\n",
    "    additional_col_df = pd.DataFrame({k: [None for _ in range(n)] for k in set_diff})\n",
    "    new_df = pd.concat([df, additional_col_df], axis=1)\n",
    "    return new_df\n",
    "\n",
    "def concat_dfs(dataframes: list):\n",
    "    return pd.concat(dataframes, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23892304-fdeb-4dac-ad93-e84478fcd8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: 42052178, Num_files: 6\n",
      "(15000, 53)\n",
      "(15000, 56)\n",
      "(15000, 57)\n",
      "(15000, 61)\n",
      "(15000, 54)\n",
      "(5911, 348)\n",
      "Subject: 50311906, Num_files: 3\n",
      "(205, 48)\n",
      "(15000, 88)\n",
      "(10835, 81)\n",
      "Subject: 61179686, Num_files: 5\n",
      "(15000, 258)\n",
      "(15000, 16)\n",
      "(15000, 16)\n",
      "(15000, 16)\n",
      "(1272, 18)\n",
      "Subject: 66773091, Num_files: 6\n",
      "(15000, 16)\n",
      "(15000, 16)\n",
      "(15000, 16)\n",
      "(15000, 16)\n",
      "(15000, 16)\n",
      "(10132, 16)\n"
     ]
    }
   ],
   "source": [
    "ndt_subjects = [x for x in subject_dirs if x.subject_id in no_device_but_treatment]\n",
    "ndt_dfs = {}\n",
    "for subj in ndt_subjects:\n",
    "    print(f\"Subject: {subj.subject_id}, Num_files: {len(subj.treatments)}\")\n",
    "    ndt_dfs.update({subj.subject_id: [pd.read_csv(file, dtype=str) for file in subj.treatments]}) \n",
    "    for df in ndt_dfs[subj.subject_id]:\n",
    "        print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "252d6504-e2fa-4980-9c53-eb27bc0bea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list = [df for df in ndt_dfs[\"42052178\"]]\n",
    "# superset = define_column_superset(df_list)\n",
    "# print('supersetting')\n",
    "# superset_dfs = [apply_superset(df, superset) for df in df_list]\n",
    "# print('concatting')\n",
    "# union_df = concat_dfs(superset_dfs)\n",
    "# # union_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d97b3245-ae73-4993-b1fc-bad60f2bfc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unioned in: 0:00:01.662886\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>absolute</th>\n",
       "      <th>carbs</th>\n",
       "      <th>_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>NSCLIENT_ID</th>\n",
       "      <th>rate</th>\n",
       "      <th>eventType</th>\n",
       "      <th>enteredBy</th>\n",
       "      <th>insulin</th>\n",
       "      <th>...</th>\n",
       "      <th>raw_duration/_type</th>\n",
       "      <th>bolus/appended/0/data/5/age</th>\n",
       "      <th>square/appended/0/data/0/amount</th>\n",
       "      <th>changed/insulin_sensitivies/3/_offset</th>\n",
       "      <th>raw_rate/appended/0/data/1/amount</th>\n",
       "      <th>preBolus</th>\n",
       "      <th>changed/insulin_sensitivies/7/offset</th>\n",
       "      <th>stale/insulin_sensitivies/0/_offset</th>\n",
       "      <th>raw_rate/_body</th>\n",
       "      <th>stale/insulin_sensitivies/7/sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-10T23:40:47Z</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5cd69da3131daf5594f593e1</td>\n",
       "      <td>120</td>\n",
       "      <td>1557531647994</td>\n",
       "      <td>0</td>\n",
       "      <td>Temp Basal</td>\n",
       "      <td>S6MIS6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-10T23:39:03Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5cd69da3131daf5594f593e3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1557531661775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Correction Bolus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-10T23:30:53Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5cd69da3131daf5594f593e2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1557531053916</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Temp Basal</td>\n",
       "      <td>S6MIS6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-10T23:29:01Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5cd69da3131daf5594f593e4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1557531068046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Correction Bolus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-10T23:17:56Z</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5cd69d5a131daf5594f593cc</td>\n",
       "      <td>82</td>\n",
       "      <td>1557530276855</td>\n",
       "      <td>0</td>\n",
       "      <td>Temp Basal</td>\n",
       "      <td>S6MIS6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 353 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             created_at absolute carbs                       _id duration  \\\n",
       "0  2019-05-10T23:40:47Z        0   NaN  5cd69da3131daf5594f593e1      120   \n",
       "1  2019-05-10T23:39:03Z      NaN   NaN  5cd69da3131daf5594f593e3      NaN   \n",
       "2  2019-05-10T23:30:53Z      NaN   NaN  5cd69da3131daf5594f593e2      NaN   \n",
       "3  2019-05-10T23:29:01Z      NaN   NaN  5cd69da3131daf5594f593e4      NaN   \n",
       "4  2019-05-10T23:17:56Z        0   NaN  5cd69d5a131daf5594f593cc       82   \n",
       "\n",
       "     NSCLIENT_ID rate         eventType enteredBy insulin  ...  \\\n",
       "0  1557531647994    0        Temp Basal    S6MIS6     NaN  ...   \n",
       "1  1557531661775  NaN  Correction Bolus       NaN     0.4  ...   \n",
       "2  1557531053916  NaN        Temp Basal    S6MIS6     NaN  ...   \n",
       "3  1557531068046  NaN  Correction Bolus       NaN     0.2  ...   \n",
       "4  1557530276855    0        Temp Basal    S6MIS6     NaN  ...   \n",
       "\n",
       "  raw_duration/_type bolus/appended/0/data/5/age  \\\n",
       "0               None                        None   \n",
       "1               None                        None   \n",
       "2               None                        None   \n",
       "3               None                        None   \n",
       "4               None                        None   \n",
       "\n",
       "  square/appended/0/data/0/amount changed/insulin_sensitivies/3/_offset  \\\n",
       "0                            None                                  None   \n",
       "1                            None                                  None   \n",
       "2                            None                                  None   \n",
       "3                            None                                  None   \n",
       "4                            None                                  None   \n",
       "\n",
       "  raw_rate/appended/0/data/1/amount preBolus  \\\n",
       "0                              None     None   \n",
       "1                              None     None   \n",
       "2                              None     None   \n",
       "3                              None     None   \n",
       "4                              None     None   \n",
       "\n",
       "  changed/insulin_sensitivies/7/offset stale/insulin_sensitivies/0/_offset  \\\n",
       "0                                 None                                None   \n",
       "1                                 None                                None   \n",
       "2                                 None                                None   \n",
       "3                                 None                                None   \n",
       "4                                 None                                None   \n",
       "\n",
       "  raw_rate/_body stale/insulin_sensitivies/7/sensitivity  \n",
       "0           None                                    None  \n",
       "1           None                                    None  \n",
       "2           None                                    None  \n",
       "3           None                                    None  \n",
       "4           None                                    None  \n",
       "\n",
       "[5 rows x 353 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [df.reset_index(drop=True) for df in superset_dfs]\n",
    "start = datetime.now()\n",
    "union_df = pd.concat(test, axis = 0)\n",
    "end = datetime.now()\n",
    "print(\"Unioned in:\", end-start)\n",
    "union_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12686d-89e5-4359-8ee7-a0701a327242",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prints columns\n",
    "# cols = []\n",
    "# for i,col in enumerate(union_df.columns):\n",
    "#     cols.append(col)\n",
    "#     if i % 5 == 0:\n",
    "#         print(cols)\n",
    "#         cols = []\n",
    "union_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5abdb9-f2ba-4241-9ff1-97f587cd03cf",
   "metadata": {},
   "source": [
    "We can see that these treatment tables hold a lot of relevant information. We'd love to keep this info if possible. We'll have to do a lot of work to process the though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84347b9-ee0d-4a2a-9283-869ae8402d8d",
   "metadata": {},
   "source": [
    "## Evaluate Treatment Uniqueness\n",
    "In Nightscout join analysis, we saw that a union of treatments with duplicates dropped exactly matched a device status file. Let's see how common that is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ead9222-a725-4fb8-bb5b-edb86eb804cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subject:\n",
    "    \n",
    "    def __init__(self, subject_id, subject_path, treatment_files, device_status_files, entries_files):\n",
    "        self.subject_id = subject_id\n",
    "        self.subject_path = subject_path\n",
    "        \n",
    "        self.treatment_files = treatment_files\n",
    "        self.treatment_shapes = None\n",
    "        self.treatment_df = None\n",
    "        \n",
    "        self.device_status_files = device_status_files\n",
    "        self.device_status_shapes = None\n",
    "        self.device_status_df = None\n",
    "        \n",
    "        self.entries_files = entries_files\n",
    "        self.entries_shapes = None\n",
    "        self.entries_df = None \n",
    "        \n",
    "    \n",
    "    def get_device_status_shapes(self):\n",
    "        if self.device_status_shapes is not None:\n",
    "            return self.device_status_shapes\n",
    "        else:\n",
    "            self.device_status_shapes = [pd.read_csv(file, low_memory=False).shape for file in self.device_status_files]\n",
    "            return self.device_status_shapes\n",
    "    \n",
    "    def get_device_status_df(self):\n",
    "        if self.device_status_df is not None:\n",
    "            return self.device_status_df\n",
    "        else:\n",
    "            device_status_dfs = [pd.read_csv(file, low_memory=False).reset_index(drop=True) for file in self.device_status_files]\n",
    "            self.device_status_df = pd.concat(device_status_dfs, axis=0)\n",
    "            return self.device_status_df \n",
    "    \n",
    "    def get_treatment_shapes(self):\n",
    "        if self.treatment_shapes is not None:\n",
    "            return self.treatment_shapes\n",
    "        else:\n",
    "            self.treatment_shapes = [pd.read_csv(file, low_memory=False).shape for file in self.treatment_files]\n",
    "            return self.treatment_shapes\n",
    "        \n",
    "    def get_treatment_df(self):\n",
    "        if self.treatment_df is not None:\n",
    "            return self.treatment_df\n",
    "        else:\n",
    "            treatment_dfs = [pd.read_csv(file, low_memory=False).reset_index(drop=True) for file in self.treatment_files]\n",
    "            self.treatment_df = pd.concat(treatment_dfs, axis=0)\n",
    "            return self.treatment_df\n",
    "        \n",
    "    def get_entries_shapes(self):\n",
    "        if self.entries_shapes is not None:\n",
    "            return self.entries_shapes\n",
    "        else:\n",
    "            self.entries_shapes = [pd.read_csv(file, low_memory=False).shape for file in self.entries_files]\n",
    "            return self.entries_shapes\n",
    "    \n",
    "    def get_entries_df(self):\n",
    "        if self.entries_df is not None:\n",
    "            return self.entries_df\n",
    "        else:\n",
    "            self.entries_dfs = [pd.read_csv(file, low_memory=False).reset_index(drop=True) for file in self.entries_files]\n",
    "            return self.entries_df\n",
    "            \n",
    "    def check_equivalence(self, treatment_df=None, check_unique=False):\n",
    "        \"\"\"Check rather treatment data is a subset of or equivalent to device status data\"\"\"\n",
    "        if treatment_df is None:\n",
    "            treatment_df = self.get_treatment_df()\n",
    "        ds_df = self.get_device_status_df()\n",
    "        if treatment_df.shape == ds_df.shape:\n",
    "            treat_array = self.to_string_and_numpy(treatment_df)\n",
    "            ds_array = self.to_string_and_numpy(ds_df)\n",
    "            if np.array_equal(treat_array, ds_array):\n",
    "                print(\"Equivalent full treatment and device status\")\n",
    "                return 1\n",
    "            else:\n",
    "                print(\"Equivalent full treatment and device status shape but not elements\")\n",
    "                return 0\n",
    "        elif treatment_df.shape in self.get_device_status_shapes():\n",
    "            idx = self.get_device_status_shapes().index(treatment_df.shape)\n",
    "            ds_df = pd.read_csv(self.device_status_files[idx], low_memory=False)\n",
    "            ds_array = self.to_string_and_numpy(ds_df)\n",
    "            treat_array = self.to_string_and_numpy(treatment_df)\n",
    "            if np.array_equal(treat_array, ds_array):\n",
    "                print(\"Treatment equivalent to subset of device status\")\n",
    "                return 1 \n",
    "            else:\n",
    "                print(\"Treatment has equivalent shape to subset of device status but different elements\")\n",
    "                return 0 \n",
    "        elif not check_unique:\n",
    "            # Make a recursive check on the treatment dataframe with duplicates dropped \n",
    "            print('here')\n",
    "            self.check_equivalence(treatment_df.drop_duplicates(), check_unique=True)\n",
    "        else:\n",
    "            print(\"Treatment data is not a copy of device status data\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_string_and_numpy(df):\n",
    "        return df.as_type('str').to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc7e30-bbca-4b43-8f55-c7fd9831e0b8",
   "metadata": {},
   "source": [
    "**Subject 01352464: No duplicated treatment and device status**\n",
    "Here, we validate that subject 01352464 does not have any duplicates from the treatment data frame and that the check equivalence function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d85c413e-76aa-48ec-8f3f-134aeee05b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = subject_dirs[3]\n",
    "sub_3 = Subject(subject.subject_id, subject.path, subject.treatments, subject.devicestatus, subject.entries)\n",
    "# ds_shapes = sub_1.get_device_status_shapes()\n",
    "# treat_shapes = sub_1.get_treatment_shapes()\n",
    "treatment_df = sub_1.get_treatment_df()\n",
    "ds_df = sub_1.get_device_status_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccd5a9f7-54dd-493b-ba3e-5f5f51626057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140527, 403)\n",
      "(167799, 1222)\n"
     ]
    }
   ],
   "source": [
    "print(treatment_df.shape)\n",
    "print(ds_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae701745-73ac-43ef-aaf5-15a54adc8fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "for t_col in treatment_df.columns:\n",
    "    if t_col in ds_df.columns:\n",
    "        test.append((t_col, 1))\n",
    "    else:\n",
    "        test.append((t_col, 0))\n",
    "print(sum([x[1] for x in test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bfdce5d-ed29-4b92-8013-508d741dab31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('created_at', 1), ('_id', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [x for x in test if x[1] ==1]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4496741d-4bd5-4265-a712-dd161757c3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "no unique match\n"
     ]
    }
   ],
   "source": [
    "subject = subject_dirs[3]\n",
    "sub_3 = Subject(subject.subject_id, subject.path, subject.treatments, subject.devicestatus, subject.entries)\n",
    "sub_3.check_equivalence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7ce9dbf-2bca-426d-9c41-bbb8710a94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up variables\n",
    "del sub_3\n",
    "del treatment_df\n",
    "del ds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb10f0a-f090-4016-afdf-1340212f53ac",
   "metadata": {},
   "source": [
    "**Subject 00221634: No duplicated treatment and device status**\n",
    "In NightScoutJoinAnalysis, we saw that subject 00221634 has duplicate treatment and device status data. Evaluate rather `check_equivalence()` identifies the duplicate.\n",
    "\n",
    "Written after running the below: For reasons, it appears that this data isn't as duplicated as I found in NightScoutJoinAnalysis. So, I'm just going to full join everything. Then, we can compress data across the relavent columns to shrink the dataset horizontally. Then, we'll still have multiple rows for each blood glucose entry. We can then groupby blood glucose entry and perform aggregations on the relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de9c679d-42c7-455b-bd9a-9f86744d6604",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = [s for s in subject_dirs if s.subject_id==\"00221634\"][0]\n",
    "subject\n",
    "sub_3 = Subject(subject.subject_id, subject.path, subject.treatments, subject.devicestatus, subject.entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6248bb3-4b12-44f1-84e0-714d213a5859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/spenc/Documents/Berkeley/Capstone/n=183_OpenAPS_Data_Commons_August_2021_UNZIPPED/00221634/direct-sharing-31/00221634_treatments_2018-03-01_to_2018-08-05_csv/00221634_treatments_2018-03-01_to_2018-08-05_aa.csv', 'C:/Users/spenc/Documents/Berkeley/Capstone/n=183_OpenAPS_Data_Commons_August_2021_UNZIPPED/00221634/direct-sharing-31/00221634_treatments_2018-03-01_to_2018-08-05_csv/00221634_treatments_2018-03-01_to_2018-08-05_ab.csv']\n"
     ]
    }
   ],
   "source": [
    "print(subject.treatments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2b784e5-aa45-4fe6-92a3-ca44742de6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "Treatment data is not a copy of device status data\n"
     ]
    }
   ],
   "source": [
    "sub_3.check_equivalence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78baf880-efeb-489f-b1aa-cfae8cdb60c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26689, 59)\n",
      "(53877, 654)\n"
     ]
    }
   ],
   "source": [
    "treatment_df = sub_3.get_treatment_df()\n",
    "print(treatment_df.shape)\n",
    "ds_df = sub_3.get_device_status_df()\n",
    "print(ds_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "957992ed-7e44-41fe-bca6-cc1d27c8a74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2018-08-04 23:47:05+00:00\n",
       "1       2018-08-04 23:01:08+00:00\n",
       "2       2018-08-04 22:10:26+00:00\n",
       "3       2018-08-04 22:10:26+00:00\n",
       "4       2018-08-04 21:49:05+00:00\n",
       "                   ...           \n",
       "11684   2018-03-01 17:37:00+00:00\n",
       "11685   2018-03-01 15:01:00+00:00\n",
       "11686   2018-03-01 14:17:00+00:00\n",
       "11687   2018-03-01 12:17:51+00:00\n",
       "11688   2018-03-01 06:37:58+00:00\n",
       "Name: created_at, Length: 26689, dtype: datetime64[ns, UTC]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(treatment_df.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de4477c4-fee7-4255-bae7-d5d7855ee50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26689"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(treatment_df._id.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1bb1d-6775-4eb5-82ee-08ad8d52b4bb",
   "metadata": {},
   "source": [
    "## Verify all entries have the same number of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c177f3b5-7527-4137-8420-a470f3d5e0f5",
   "metadata": {},
   "source": [
    "This verifies all entry files have the same shape and can be naively unioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b0d3e-a134-4e48-bcbc-72c09208741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_objs = []\n",
    "for x in subject_dirs:\n",
    "    subject_objs.append(\n",
    "        Subject(x.subject_id, x.path, x.treatments, x.devicestatus, x.entries)\n",
    "    )\n",
    "for x in subject_objs:\n",
    "    shapes = x.get_entries_shapes()\n",
    "    for shape in shapes:\n",
    "        if shape[1] != 2:\n",
    "            print(f\"{x.subject_id} has malconformed shape. {shapes}\")\n",
    "            break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e414f-9736-405a-8686-5ec4b935c93a",
   "metadata": {},
   "source": [
    "## Raw join of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81b2268c-d5ac-4b3c-90b1-8ae9a068fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subject:\n",
    "    \n",
    "    def __init__(self, subject_id, subject_path, treatment_files, device_status_files, entries_files):\n",
    "        self.subject_id = subject_id\n",
    "        self.subject_path = subject_path\n",
    "        \n",
    "        self.treatment_files = treatment_files\n",
    "        self.treatment_shapes = None\n",
    "        self.treatment_df = None\n",
    "        \n",
    "        self.device_status_files = device_status_files\n",
    "        self.device_status_shapes = None\n",
    "        self.device_status_df = None\n",
    "        \n",
    "        self.entries_files = entries_files\n",
    "        self.entries_shapes = None\n",
    "        self.entries_df = None \n",
    "        \n",
    "        self.join_table = None \n",
    "        \n",
    "    def get_device_status_shapes(self):\n",
    "        if self.device_status_shapes is not None:\n",
    "            return self.device_status_shapes\n",
    "        else:\n",
    "            self.device_status_shapes = [pd.read_csv(file, low_memory=False).shape for file in self.device_status_files]\n",
    "            return self.device_status_shapes\n",
    "    \n",
    "    def get_device_status_df(self):\n",
    "        if self.device_status_df is not None:\n",
    "            return self.device_status_df\n",
    "        else:\n",
    "            device_status_dfs = [pd.read_csv(file, low_memory=False).reset_index(drop=True) for file in self.device_status_files]\n",
    "            self.device_status_df = pd.concat(device_status_dfs, axis=0)\n",
    "            self.device_status_df['timestamp'] = pd.to_datetime(self.device_status_df['created_at'])\n",
    "            self.device_status_df['devicestatusid'] = [i for i in range(len(self.device_status_df))]\n",
    "            return self.device_status_df \n",
    "    \n",
    "    def get_treatment_shapes(self):\n",
    "        if self.treatment_shapes is not None:\n",
    "            return self.treatment_shapes\n",
    "        else:\n",
    "            self.treatment_shapes = [pd.read_csv(file, low_memory=False).shape for file in self.treatment_files]\n",
    "            return self.treatment_shapes\n",
    "        \n",
    "    def get_treatment_df(self):\n",
    "        if self.treatment_df is not None:\n",
    "            return self.treatment_df\n",
    "        else:\n",
    "            treatment_dfs = [pd.read_csv(file, low_memory=False).reset_index(drop=True) for file in self.treatment_files]\n",
    "            self.treatment_df = pd.concat(treatment_dfs, axis=0)\n",
    "            try:\n",
    "                self.treatment_df['timestamp'] = pd.to_datetime(self.treatment_df[\"created_at\"])\n",
    "            except ValueError:\n",
    "                # Somewhat randomly, for unclear reasons, we receive the following error:\n",
    "                # ValueError: cannot reindex from a duplicate axis\n",
    "                # Resetting index seems to resolve this.\n",
    "                self.treatment_df = self.treatment_df.reset_index()\n",
    "                self.treatment_df['timestamp'] = pd.to_datetime(self.treatment_df[\"created_at\"])\n",
    "            self.treatment_df['treatmentid'] = [i for i in range(len(self.treatment_df))]\n",
    "            return self.treatment_df\n",
    "        \n",
    "    def get_entries_shapes(self):\n",
    "        if self.entries_shapes is not None:\n",
    "            return self.entries_shapes\n",
    "        else:\n",
    "            self.entries_shapes = [pd.read_csv(file, low_memory=False).shape for file in self.entries_files]\n",
    "            return self.entries_shapes\n",
    "    \n",
    "    def get_entries_df(self):\n",
    "        if self.entries_df is not None:\n",
    "            return self.entries_df\n",
    "        else:\n",
    "            entries_dfs = [pd.read_csv(file, low_memory=False, header=None).reset_index(drop=True) for file in self.entries_files]\n",
    "            self.entries_df = pd.concat(entries_dfs, axis=0)\n",
    "            self.entries_df.columns = [\"time\", \"bg\"]\n",
    "            self.entries_df['timestamp'] = pd.to_datetime(self.entries_df['time'])\n",
    "            self.entries_df['entryid']  = [i for i in range(len(self.entries_df))]\n",
    "            return self.entries_df\n",
    "\n",
    "    def temporal_join(self):\n",
    "        \n",
    "        # Load tables and convert relevant columns to date times\n",
    "        entries = self.get_entries_df()\n",
    "        treatments = self.get_treatment_df()\n",
    "        device_status = self.get_device_status_df()\n",
    "        \n",
    "        # Store timestamp and entries in zipped list\n",
    "        timestamp_keys = entries['timestamp'].to_list()\n",
    "        entry_id_list = entries['entryid'].to_list()\n",
    "        zipped = list(zip(timestamp_keys, entry_id_list))\n",
    "\n",
    "        # fill in standard python dictionary with entry data; convert to SortedDict sorted on entry timestamps\n",
    "        index_dict = SortedDict({timestamp: (entry_id, {\"device_status\": [], \"treatment\": []}) for timestamp, entry_id in zipped})\n",
    "\n",
    "        # Generate list of tuples for (devicetimestamp, deviceid) \n",
    "        device_tuples = list(zip(device_status['timestamp'], device_status['devicestatusid']))\n",
    "        \n",
    "        # Generate list of tuples for (devicetimestamp, deviceid) \n",
    "        treatments_tuples = list(zip(treatments['timestamp'], treatments['treatmentid']))\n",
    "        \n",
    "        # Set constants from index_dict\n",
    "        index_keys = index_dict.keys()\n",
    "        max_idx = index_dict.index(index_keys[len(index_keys)-1])\n",
    "        \n",
    "        for comparison_timestamp, comparison_id in device_tuples:\n",
    "            # Left idx is the index of the entry timestamp the comparison timestamp is less than or equal to \n",
    "            left_idx = index_dict.bisect_left(comparison_timestamp) \n",
    "            \n",
    "            # Assign comparison timestamps greater than the last entry to the last entry\n",
    "            # (Comparisons < min(entry timestamp) will naturally be joined to min(entry timestamp))\n",
    "            if left_idx >= max_idx:\n",
    "                left_idx = max_idx\n",
    "            \n",
    "            # Get the index_dict key associated with the bisect_left operation\n",
    "            assignment_key = index_keys[left_idx]\n",
    "            \n",
    "            # Assign the comparison_id to the assignment key of the index_dict\n",
    "            index_dict[assignment_key][1]['device_status'].append(comparison_id)\n",
    "            \n",
    "        # Equivalent to the above for-loop but for treatments_tuples    \n",
    "        for comparison_timestamp, comparison_id in device_tuples:\n",
    "            # Left idx is the index of the entry timestamp the comparison timestamp is less than or equal to \n",
    "            left_idx = index_dict.bisect_left(comparison_timestamp) \n",
    "\n",
    "            # Assign comparison timestamps greater than the last entry to the last entry\n",
    "            # (Comparisons < min(entry timestamp) will naturally be joined to min(entry timestamp))\n",
    "            if left_idx >= max_idx:\n",
    "                left_idx = max_idx\n",
    "\n",
    "            # Get the index_dict key associated with the bisect_left operation\n",
    "            assignment_key = index_keys[left_idx]\n",
    "\n",
    "            # Assign the comparison_id to the assignment key of the index_dict\n",
    "            index_dict[assignment_key][1]['treatment'].append(comparison_id)\n",
    "        \n",
    "        return index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25cd7485-5433-4efd-b492-813e98b6032e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02199852\n"
     ]
    }
   ],
   "source": [
    "# subject = [s for s in subject_dirs if s.subject_id==\"00221634\"][0]\n",
    "subject = subject_dirs[5]\n",
    "sub = Subject(subject.subject_id, subject.path, subject.treatments, subject.devicestatus, subject.entries)\n",
    "print(sub.subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d5093a3-8822-4b70-9d7c-0a24e9ece7b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'treatment_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19416/1331761351.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtreatment_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtreatment_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtreatment_df\u001b[0m\u001b[1;33m=\u001b[0m  \u001b[0mtreatment_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtreatment_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'treatment_df' is not defined"
     ]
    }
   ],
   "source": [
    "# print(treatment_df.columns.is_unique)\n",
    "# print(treatment_df.index.is_unique)\n",
    "# treatment_df=  treatment_df.reset_index()\n",
    "# print(treatment_df.index.is_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04203368-e068-47ea-8d35-d8a0625f8ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment_df = sub.get_treatment_df()\n",
    "treatment_df['created_at'] = pd.to_datetime(treatment_df.created_at)\n",
    "treatment_df.reset_index().index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36269a73-8f53-4385-ba64-1d36d339bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test= sub.temporal_join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e745ee05-4d94-4adb-b69c-84a357064bd9",
   "metadata": {},
   "source": [
    "In the below cell, it's curious that we see the same treatment and device status ID's given that these are independently created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f00b99c3-6aa5-4471-a0b5-0e118a1ee264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49331, {'device_status': [121681, 121682, 121683], 'treatment': [121681, 121682, 121683]})\n",
      "(49330, {'device_status': [121675, 121676, 121677, 121678, 121679, 121680], 'treatment': [121675, 121676, 121677, 121678, 121679, 121680]})\n",
      "(49329, {'device_status': [121667, 121668, 121669, 121670, 121671, 121672, 121673, 121674], 'treatment': [121667, 121668, 121669, 121670, 121671, 121672, 121673, 121674]})\n",
      "(49328, {'device_status': [121663, 121664, 121665, 121666], 'treatment': [121663, 121664, 121665, 121666]})\n",
      "(49327, {'device_status': [121654, 121655, 121656, 121657, 121658, 121659, 121660, 121661, 121662], 'treatment': [121654, 121655, 121656, 121657, 121658, 121659, 121660, 121661, 121662]})\n",
      "(49326, {'device_status': [121645, 121646, 121647, 121648, 121649, 121650, 121651, 121652, 121653], 'treatment': [121645, 121646, 121647, 121648, 121649, 121650, 121651, 121652, 121653]})\n",
      "(49325, {'device_status': [121637, 121638, 121639, 121640, 121641, 121642, 121643, 121644], 'treatment': [121637, 121638, 121639, 121640, 121641, 121642, 121643, 121644]})\n",
      "(49324, {'device_status': [121630, 121631, 121632, 121633, 121634, 121635, 121636], 'treatment': [121630, 121631, 121632, 121633, 121634, 121635, 121636]})\n",
      "(49323, {'device_status': [121622, 121623, 121624, 121625, 121626, 121627, 121628, 121629], 'treatment': [121622, 121623, 121624, 121625, 121626, 121627, 121628, 121629]})\n",
      "(49322, {'device_status': [121614, 121615, 121616, 121617, 121618, 121619, 121620, 121621], 'treatment': [121614, 121615, 121616, 121617, 121618, 121619, 121620, 121621]})\n",
      "(49321, {'device_status': [121607, 121608, 121609, 121610, 121611, 121612, 121613], 'treatment': [121607, 121608, 121609, 121610, 121611, 121612, 121613]})\n",
      "(49320, {'device_status': [121599, 121600, 121601, 121602, 121603, 121604, 121605, 121606], 'treatment': [121599, 121600, 121601, 121602, 121603, 121604, 121605, 121606]})\n",
      "(49319, {'device_status': [121591, 121592, 121593, 121594, 121595, 121596, 121597, 121598], 'treatment': [121591, 121592, 121593, 121594, 121595, 121596, 121597, 121598]})\n",
      "(49318, {'device_status': [121583, 121584, 121585, 121586, 121587, 121588, 121589, 121590], 'treatment': [121583, 121584, 121585, 121586, 121587, 121588, 121589, 121590]})\n",
      "(49317, {'device_status': [121575, 121576, 121577, 121578, 121579, 121580, 121581, 121582], 'treatment': [121575, 121576, 121577, 121578, 121579, 121580, 121581, 121582]})\n",
      "(49316, {'device_status': [121567, 121568, 121569, 121570, 121571, 121572, 121573, 121574], 'treatment': [121567, 121568, 121569, 121570, 121571, 121572, 121573, 121574]})\n",
      "(49315, {'device_status': [121559, 121560, 121561, 121562, 121563, 121564, 121565, 121566], 'treatment': [121559, 121560, 121561, 121562, 121563, 121564, 121565, 121566]})\n",
      "(49314, {'device_status': [121552, 121553, 121554, 121555, 121556, 121557, 121558], 'treatment': [121552, 121553, 121554, 121555, 121556, 121557, 121558]})\n",
      "(49313, {'device_status': [121544, 121545, 121546, 121547, 121548, 121549, 121550, 121551], 'treatment': [121544, 121545, 121546, 121547, 121548, 121549, 121550, 121551]})\n",
      "(49312, {'device_status': [121536, 121537, 121538, 121539, 121540, 121541, 121542, 121543], 'treatment': [121536, 121537, 121538, 121539, 121540, 121541, 121542, 121543]})\n",
      "(49311, {'device_status': [121529, 121530, 121531, 121532, 121533, 121534, 121535], 'treatment': [121529, 121530, 121531, 121532, 121533, 121534, 121535]})\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for k,v in test.items():\n",
    "    if len(v[1][\"device_status\"]) and len(v[1]['treatment']):\n",
    "        print(v)\n",
    "        i +=1\n",
    "    if i > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d51e73-db63-4dff-935b-934667a4ca2b",
   "metadata": {},
   "source": [
    "Based on the cell below, it appears the device status and treatment ID's are never different where there are matched values. It appears that device status and treatment data both align with a portion of the entries data, AND they align with the same portion of the entries data. Therefore, we have some subset of entries data with treatment and device status data most of the time. (I re-ran this process for several subjects). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4349937b-0ecc-46a9-915f-b77eecaf858c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51753\n",
      "not empty matched:  39469\n",
      "not empty unmatched: 0\n",
      "matched empty: 12284\n"
     ]
    }
   ],
   "source": [
    "# Evalute \n",
    "i = 0\n",
    "matched =0 \n",
    "not_empty_unmatched = 0\n",
    "matched_empty = 0\n",
    "for k,v in test.items():\n",
    "    if (v[1][\"device_status\"] == v[1]['treatment']) and len(v[1][\"device_status\"]) >=1:\n",
    "        matched += 1 \n",
    "    elif len(v[1][\"device_status\"]) >1 or len(v[1][\"treatment\"]) >1:\n",
    "        print(\"Not matched \\n\", v)\n",
    "        not_matched +=1\n",
    "    elif (v[1][\"device_status\"] == v[1]['treatment']) and len(v[1][\"device_status\"]) <1:\n",
    "        matched_empty +=1 \n",
    "    else:\n",
    "        print(v)\n",
    "#     if i > 10:\n",
    "#         break\n",
    "print(len(test))\n",
    "print(\"not empty matched: \", matched)\n",
    "print(\"not empty unmatched:\", not_empty_unmatched)\n",
    "print(\"matched empty:\" ,matched_empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1516e7d0-1402-40b7-a5cb-74d522241bc0",
   "metadata": {},
   "source": [
    "Now, let's look into making the join dataframe from the temporal_join output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "590438eb-c7d0-4a75-9127-3f607b0f32d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36686, {'device_status': [94551], 'treatment': [94551]})\n",
      "(36685, {'device_status': [94550], 'treatment': [94550]})\n",
      "(36684, {'device_status': [94549], 'treatment': [94549]})\n",
      "(36683, {'device_status': [94548], 'treatment': [94548]})\n",
      "(36682, {'device_status': [94547], 'treatment': [94547]})\n"
     ]
    }
   ],
   "source": [
    "keys = test.keys()\n",
    "for k in keys[15000:15005]:\n",
    "    print(test[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ecf3dda-2957-4dc6-9493-6cf7a4780c04",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'devices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19416/2458408744.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mentry_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mds_and_treatment_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtreatments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m join_dict = {\"entryid\": [i[0] for  i in ds_and_treatment_id],\n\u001b[0;32m     11\u001b[0m              \u001b[1;34m\"devicestatusid\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mds_and_treatment_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'devices' is not defined"
     ]
    }
   ],
   "source": [
    "ds_and_treatment_id = []\n",
    "for k in keys:\n",
    "    device_ids = test[k][1]['device_status']\n",
    "    treatment_ids = test[k][1]['treatment']\n",
    "    if len(device_ids) != len(treatment_ids):\n",
    "        raise Exception(f\"Nonconformant device and treatment lengths ({len(device_ids)}, {len(treatment_ids)}\")\n",
    "    \n",
    "    entry_ids = [test[k][0] for _ in range(len(device_ids))]\n",
    "    ds_and_treatment_id.extend(list(zip(entry_ids, devices, treatments)))\n",
    "join_dict = {\"entryid\": [i[0] for  i in ds_and_treatment_id],\n",
    "             \"devicestatusid\": [i[1] for i in ds_and_treatment_id],\n",
    "             \"treatmentid\": [i[2] for i in ds_and_treatment_id]}\n",
    "join_df = pd.DataFrame(join_dict)\n",
    "join_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbfe33f-a639-40d3-8cb9-077f706949c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_df.loc[entry_df['entryid'] == 51690, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cdcbd7-e734-4b76-b282-5c6f650ba6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df = sub.get_device_status_df()\n",
    "treat_df = sub.get_treatment_df()\n",
    "entry_df = sub.get_entries_df()\n",
    "join_df.merge(entry_df, how='left', left_on ='entryid', right_on='entryid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db81d88-4c63-4f56-9d21-d196b6b9a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ad106-e489-420f-aa1b-d7dca7202c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = (join_df\n",
    "          .merge(entry_df, how='left', left_on ='entryid', right_on='entryid', suffixes=(\"_x\",\"_ent\"))\n",
    "          .merge(ds_df, how='left', left_on=\"devicestatusid\", right_on=\"devicestatusid\", suffixes=(\"_y\",\"_ds\"))\n",
    "          .merge(treat_df, how='left', left_on=\"treatmentid\", right_on=\"treatmentid\", suffixes=(\"_z\",\"_tre\"))\n",
    "         )\n",
    "joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "012177b8-344f-4159-a06b-47fb0c40d78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65104, 546)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "e8e83d4e-f084-450d-922d-ae62b0fed985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entryid               0\n",
       "devicestatusid        0\n",
       "treatmentid           0\n",
       "time                  0\n",
       "bg                    0\n",
       "                  ...  \n",
       "ratio             65104\n",
       "units             65104\n",
       "glucoseType       65104\n",
       "glucose           65104\n",
       "isAnnouncement    65104\n",
       "Length: 546, dtype: int64"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.isna().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
